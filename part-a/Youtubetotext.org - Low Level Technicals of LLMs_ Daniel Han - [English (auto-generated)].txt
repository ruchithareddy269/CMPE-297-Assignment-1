Video title: Low Level Technicals of LLMs: Daniel Han
    
Video URL: https://youtube.com/watch?v=pRM_P6UfdIc
    
Video language: English (auto-generated)
    
--------------------------------

[Music] welcome to the AI Engineers worldair um this is the first Workshop um there's a few other running but like thanks for coming um we just arrived from like Australia with my brother um I think it's over there somewhere but yes we just came here um yes we didn't know a lot of stuff about SF and I think maybe the US is a bit different from Australia um but yeah we're very excited to be here so like we're going to stay here for like a few months so if you want to meet up you know you can just hit me off email or Twitter or wherever um so today I'm going to be talking about lowlevel technicals of language models um yes yes I'm Daniel um but so we do have a website called un. if you want to look like look that up like um there's like cute SLS and stuff so my brother designed that we'll be using two tiny urls did it okay it's still working yeah so we'll be using two tiny URLs now so the first one is oh wait I yeah so the slides are at tiny url.com unof um hopefully that works um and there's also Q&A so like I'll be monitoring Q&A you can type any question that you like and I will be answering questions as we go um and that is at tiny.com unof QA um so if those two work so there'll be like on the bottom if you know anyone doesn't get this like this on the very bottom of the footer that we like we'll reshow these links okay just just has it doesn't work yes okay good good good okay yes so you might know me from my tweets um so GMA like kind of like released an open source Model A few months ago and we just found a few like issues and bugs for like different implementations um so like for example the first tweet that we ever did was about some sort of like the approximate jelly um bug issue and so like multiple implementations of Gemma they had different implementations some of them use exact jell some of them use approximate and like so which one is correct and so that's the question um and so like we just tweeted about this and that was like our first issue that we found we thought this was just like one issue but actually there were like many issues um and so like we found more bugs um and so I'm assuming maybe you know me from this um we did get partially recognizable through our Gemma B fixes so um yeah so today we'll be showing you how you can actually find these bgs and issues in language models and how you can actually like analyze and do this yourself um without you know um us just doing it manually ourselves and hopefully this can be like an open source project where everyone can find these issues automatically and help us to solve these issues um I always thought about like can we like automate this I don't think so it can be automated um there are actually many issues with these implement a and it's not just JMA um for example like you know we also analyed Gro um and there's like some weird things in their code like they're scaled by um 30 * 10 h x over 30 it's just it's just a clamping mechanism um you can see I also make mistakes sometimes um I like you know said it's Division and not multiplication so sometimes I misread the code um that's because like you know I spend when the code gets released I quickly try to analyze them and sometimes I mistakenly say stuff um so I have to like LGE you know showcase correction so yes I'm still human um but yes like you know we analyze lots of models and stuff like this and hopefully by the end of these workshops well actually this work okay it's actually not one Workshop it's going to be like multiple things in one so like I decided to like talk about three things but I I'll tell you about that later um hopefully you guys can like analyze and learn about how to do like find bugs and stuff like that by the end of today um so another one I did recently was like nvidia's nron I don't know if you saw this but Nvidia released a 340 billion parameter model which is extremely large I'm assuming this is in like you know in preparation like they have to release this earlier before Llama 45 billion right so they have to do this quickly um and but there are like some weird interesting things like you know they use the squared value and not the normal Swig glue and the other types um so that was very interesting they were actually the first well actually not the first but like the first big model trained to be using these like other activation functions and there's other like other weird quirks and stuff like that um and hopefully you'll be also able to like analyze like you know whenever the code comes out just read it and you'll get it um it does take some practice okay like I take like okay the first time when I read this code like it took me like many many days to read through like all these um architectures and understand exactly what they are but now it takes like 10 minutes so like I'm sure you can like just the code comes and just read it um that's the whole goal of today and also language models if you don't know they're not just like issues and bugs and Analysis of these architectures the tokenizer is a totally separate beast from language models right tokenization is like extreme extremely annoying um so like I also tweeted about like you know there's like different types of tokenization issues like mistol llama mixol like all these different types of like variants of mistol from the mistal team you know they have like different tokenization if you didn't notice um like if you so the smiley face the sun smiley face is a space and if you actually tokenize them um depending on the model you'll have different results and the question is which one is correct um unfortunately I do not know um I did asky team for this um and according to them some of them are correct and some of them are just because the mistro team forgot to update the model to the fast tokenization variant um we will be talking about this later as well um but you can see even before you can train or run the model the token Isa is broken so what are you going to do um it's a multi-pronged problem so we don't just do language models um you know like we don't you know our experience is you know a bit broader than that so we actually like I I used to actually major in maths and computer science so um yes very fun well actually I kind of did very badly in maths but anyways um yes very very fun so like SVD I don't know if anyone has anyone done normal machine learning here oh yeah yes very good there is a few people um so like the SVD I'm assuming like most people know PCA yes principal component analysis yeah okay very good data visualization it's a very powerful technique more people should know about it um SVD okay I don't I don't know if people know about SVD or like okay yes it's a bit less wellknown I'm actually a bit confused why people don't know SVD it's actually the algorithm that's one of the most important algorithms in all of like maths and computer science um it literally underpins like many applications um and extremely important um maybe we talk about that but yes I'm a huge proponent of like telling people to learn more about SVD so please do the singular value decomposition that's like the must must must must must okay like that's the most important algorithm it's because it's like one algorithm it can like spawn many other algorithms and it's like can be used for many purposes um there's also like the QR decomposition okay prob okay probably no one knows the Lu tety like there's a lot um randomize SVD yes that's extremely important as well um and yeah so we don't just do language models you can ask me any questions about you know stuff maths or computer science you have a question or real quick um so do you think um so for the neotron 340b um is it a unique architecture for because you can only use Nemo loader right now to load and train and you know do the I think that dat is just the most valuable part but we are attempting to try to convert it to a hugging face Transformer safe tensors so but we've had issues because we don't have the modeling file so um I was wondering do you think that it's similar to so at the same day they uploaded a 70b of llama 3 that's neotron as well do you think that we can get some Clues to how to build a huging face implementation yes so the question was for neotron the Cod was not released for the actual inference and what training you have to go through the Nemo training framework for MV yeah well what I mean is that I I can dump the weight yes but the code yeah I was actually planning on doing something like that but as you know we didn't have time to do that so I probably I might I might take a crack of that if is there be a QA I don't want to ask any questions oh yes then no there is so you can log questions on Q&A there is there will be Q&A so we no no yeah no you can like anyone can like raise their hands and like ask a question like I don't I'll just repeat the question um and but there is like the slider if you want to random questions I will keep monitoring um yeah and yeah so like oh yes the other one was like another paper col Laura learns less and forgets less it shows that you know fine tuning via Laura does not really work for learning new knowledge and um well it depends like it depends on how you actually read the paper like some components were incorrect they didn't actually train in all linear layers they kind of forgot a few and they also you need to do some sort of black like you know special parameters to make this work and we will also be talking about that as well but I was just trying to show you that you know we don't just do language models so we have like a whole wealth of knowledge across different Industries and stuff well not not Industries topics um and you can ask me any question that you like um so UNS slof yes um we launched this last December so I launched this my brother um it's a bit outdated the St but anyways um I think we have 11.9 something k or something I don't even know now but anyways um we launched this last December It generally makes fine tuning of um language models like llama M or jamama faster to two times faster um generally speaking and we have like 80% less memory usage now we have like some new methodologies which reduce memory even further and the trick is there is no degradation and accuracy so like you know we don't do any like approximations that's the whole purpose of doing the optimizations is we don't want to lose any accuracy um and so we do like Triton kernels so this is like from open AI then um it's a language to do like Cuda programming um essentially it's like a intermediary between the Cuda code and python itself and we'll be showing some Triton code um I don't know if we have time for like programming Triton but that'll be another topic um and yeah so like the whole purpose of unof is to make everyone be able to fine tune their language models with very bad gpus right so like Tesla t4s the free Google does anyone like people do know that Google C has free Tesla TS right yes yes right 65 Tera flops right it's actually not that bad um if you use it properly um just reminder there is a common misconception that the p100s and kagle is faster that's actually not correct um p100s I think are five times slower than Tesla TS um so although it's actually more expensive as a GPU I think I think um but it's actually slower for um so please do not select the p100s on kagle right so kagle has 30 hours for free per week gpus um and you get two Tesla t4s so that's 130 teraflops per week um 30 hours and that is actually very powerful I think that's the same as RTX 3070 or there I can't yeah I can't I can't remember exactly but um yeah so that so kagle has 30 hours for free per week Google collab it depends on how much you use normally you get four hours per day I think um I guess the pro is not that bad it's like $10 per month you can actually get like yeah it's pretty good um yeah so probably get Pro um I I do not suggest I mean you could use like runpod and Lambda labs and stuff like that I guess that's another option but we do actually share a pricing comparison so what you need to like be careful when you use gpus there is a big issue is like oh look this is the most I want to use a h100 um did you actually check how much flops that h00 provides um be careful of nvidia's marketing it's times two because it has sparcity um so just be careful of that and also you have to be careful of the flops when it's like float 8 or float 16 so just be careful of those um I do have a pricing comparison where we like normalize by like the flops um with no sparcity and we'd like look through like Lambda Labs runpod Google cab AWS um Google cloud and I think runp pod is mostly pretty good uh yes question oh the sparity so in okay so the question was why is times two for flops for the sparity feature in Nvidia um GPU so sparity the sparity feature what it does is you take 50% of the weights and make them go to zero and Nvidia essentially allows you to train this two times faster by not doing Matrix multiplications on the zeros right so you're like two * Z is just zero so like you essentially don't fire the transistors and essentially this makes it two times faster um well that's actually not that's just a higher level overview but like essentially you like compress The Matrix into this special format and then this Nvidia special format allows you to do make multiplications two times faster um yeah so it's spity so it's on h100s it's on it's on A1 100s as well so your RTX 3060 your RTX 30 series has that feature from r2x 30 the 30 series I think it has yes I think it does um so if you want to enable it the biggest issue is is that the language models most companies do not train their language models with sparity enabled if you set this like weights go to zero um you will actually ruin the behavior of the model so there are like papers which show that you can actually do like turn on the feature and then you can do fine tuning to make it work so there are actually papers which do that um so in theory you could enable this um but you know it it depends on like what models are released from the large companies um yeah I'm assuming I I think I do know if so Facebook is like they did Implement sparity in their pytorch and um X for's Library so I'm assuming they might be focused on sparity because you get times two faster um and if you know like open air you know like they keep saying oh it's two times faster hm I wonder why why is it two times faster why is it the two times faster right so like could it be sparity could it be float eight right float 8 is generally two times FAS okay not not exactly but like approximately two times faster so you know all these like things when you hear two times faster where does it come from could it be these things although we don't know but like we're just guessing um yeah any other questions okay just remember you can raise your hand or you can just like wait are there any I'm assum me there's no slided questions yet um yeah okay yeah just raise your hand um so we so for unso we do benchmarking against hugging face plus flash attention 2 um and we just show our Benchmark this is kind of old already the memory reduction is much more now um so and we do a Blog we did a blog post with them so thanks to hugging face for the collaboration um and essentially all you need to do is you know from in from UNS slof import fast language model and we try to make it as easy as possible for people to fine tun language model um and yes we'll be talking about unof a bit later um oh there is a question is it a myth or solid hypothesis that linear versus coine then short one to two EPO versus 3 to five EPO is the highly is oh is highly generalized um I think it depends um so like for training methodologies hthis that linear versus coine then of course short EPO versus long is theed best way to train any standarded model I think it depends so there are like some research papers which show that cosign or linear schedules I mean it depends to tell the truth I think it's a toss of a coin I don't think so is actually that important for the learning rate schedule a lot I think it more depends on like the data set the number of parameters um like research papers would show that if you just simply change from tide weights from unti weights to tide weights you can get like better accuracy for smaller models so I think it's so the learning rate chedu is not that important you might get accuracy plus 0.1% just train for more data there we go right get more data oh wait I press back just train for more data and I'm I'm assuming it will be similar um but to tell the truth I think it's best to like do small experiments and then like test which schedule is the best but I don't think so it's that that important um I think for the number of epo that's actually important um these like big companies um to tell the I'm not sure what llama like 15 trillion tokens is it actually 15 trillion tokens or is it like 5 trillion tokens EPO I do not know right these these questions are very important if it's 5 trilon tokens 3 that's actually very different from actually 15 trillion tokens in total but that's actually very very different but in general speaking like you know if you train for more EPO three is generally the well you know good like approximate three EPO one is actually the best for pre-training generally um you shouldn't like retrain your data like multiple times um but yeah um did you have a followup question or no well um so basically um learning rate is one was one of the big issues you fixed with the Gemma implementation oh yes so that's why I kind of that's where my Pitfall was when I was training my 2B for um Gemma and um so I actually trained it pre your fix and somehow it turned out the Benchmark and and after your fix is better than I don't know what Happ but now like one of the highest ranking oh okay know like what what do you have any theories about what could happen I trained on the Transformers broken version and then subsequent using oxel and using a um a very hard we drew forced a learning rate but it turned out surprisingly well and um we also Ed we didn't use unslot but we used like H so um so this is after the fixes that we did like it does before so before does better no well no before it was it was usable everybody else was unusable right yeah but it was usable which was surprised to me because because everybody else is was unusual but then after your fixes we are now the top my company has on the open L on Le board so you didn't even retrain you like you just no so after fixes somehow it made a b okay do you have any theories on that to be honest I do not know I think like because the fixes that we did for GMA are like multi-pronged like it's not like one fix is like nine or something so I don't know which fix caused the change there could be like all of them maybe I don't know um it's it's kind of like um just for me when I when I when I did the training right and it turned out good and then I heard from like every all my friends oh I can't do this I can't do this it's like um it just shocked me I guess okay yes that is quite shocking if like you didn't yeah we change a code like we we kind of like fix all the issues and then you don't need to retrain it and it does better okay that's okay that's a very interesting phenomenon I do not know oh yeah okay okay yeah great um yes to be honest like language models I mean these are all active areas of research please someone do a research on that yes I cannot say anything then I just read the code and fix the box I do not know so Sor yeah yeah don't worry um okay we also like do long context fine tuning so like we show that if you use a new methodology which does gradient checkpointing and offload it to system Ram you can randomly increase your contact size by four and your and the weird part is if you offload correctly to system Ram from the GPU weirdly the time of execution is just slower by 1 to 2% right so like this is very weird it's like if you can do non-blocking course and offload the GPU like memory into system Ram if you do it correctly it's not slower um some implementations unfortunately offload incorrectly um I don't want to name anyone but they offload incorrectly sometimes they offload to dis I don't know who came up with the idea of offloading to dis but anyways please try to offload to memory first and then disk right dis is extremely slow um and if you can offload to memory system Ram you can actually get away with a lot of memory usage um okay so I should have put this at the first side but anyways so today we'll be having three approximate topics um and these I wanted to make them into like three different separate topics but I guess I just mix them together whatever um so you'll be learning about lowlevel technicals of language models for example back propagation um why is why is Transformers not o of n squ o cubed for training and rather o squ and there is a lot of maths but I will try my best to reduce I think I already tried my best to reduce the maths but there are still some maps so please handle the maths um I will try my best to explain as simply as possible um that's the whole goal of the workshop so not that bad maths you will actually understand the formulas very well um just reminder I kind of nearly failed my maths in University so do not worry do not be scared um it's very fine um we talking about fast fine tuning the best tips and tricks for fine tuning how do we write the fast kernels for fine tuning um you know how do we actually make it two times faster use 70% list memory like how and with no accuracy degradation um and we'll be talking like some you know Tron opening eyes Trent language and stuff like that um and we'll be doing finding and fixing bugs um so this would be a constant phenomenon and theme how do we find and fix bugs in llama M Jama Fe um we'll be talking about mixture of experts as well oh wait maybe not but it depends on time um and we'll be doing lots of bug hunting bug fixing and more and everyone here will be a fantastic Bug Hunter and Bug fixer and we can like essentially open source our effort to fix open source models to everyone here um oh yes and we also have stickers um yes I I don't know where they are but like oh yes yeah my brother has some stickers um and we bought a few of these stickers which look pretty cute right so like you can wait my laptop has some right I put them on my laptop um and they're pretty cute I really like them my um so my brother has them we'll be handing them out um yeah as well at the end um okay so let us start um so the Transformer right so like what is the Transformer I'm assuming everyone knows what the Transformer is um does anyone not know what the Transformer is yes or no like you can simply okay yes okay so the Transformer is just an architecture that is behind all language models um so like gbd4 gbd3 you know like llama mistro Jam all these open source models what are they like what's the architecture behind them and all of them are rely on the like the transformer and the Transformer is essentially a architecture which seems to be very good for sequence modeling um so it's not just for languages it can be for any sequence modeling right so like if you know like you know Sora is a Transformer well not just a Transformer it's probably plus diffusion but like it's generally a Transformer um and there's other different types of models which doesn't have to be language modeling okay it's just sequence modeling and I probably show some pictures later um I probably should have explained a bit better but like just just just assume that Transformers are the method behind all language models okay gbd4 gbd3 gb5 okay I don't know if G who knows what gb5 is but like I'm assuming it's a Transformer um the Transformers just seem to be very good at learning new knowledge injecting knowledge into the model it seems to be very very good at changing the weights to fit the training data um which is very interesting um and the gpd2 architecture was actually very popular for the like you know most deod style Transformer that was very very popular it's still used to this day um and it kind of got reincarnated by adding extra components to it and this new architecture is called Transformer Plus+ um I don't know if people have heard of this but Transformer Plus+ is the gbd two Transformer architecture plus rope embeddings plus wigo plus RMS lay norm and with no bias um and I think it's untied weights although I'm not sure if ju I can't remember that exactly but I think it's plus untied weights and Transformer plus plus is the architecture which most people think is the best you know transform architecture for now um yes for now um there are probably like some other tweaks and little small things that Transformers still can do but like in general this would be considered the best architecture um and how does architecture look like it is just a list of maps equations um so I just wrote down the entire Transformer architecture um well this is llama 2 right so llama's Transformer architecture in one slide um and all you need to do is get some inputs like some sort of like inputs do some layer Norm do some rope embeddings do some attention plus some residual do some Lon Norm Swig glue whatever residual Leon norm and you get um and you essentially repeat this middle section L times or many times um and that is the transform architecture um okay then maybe the mass equations I'm not sure does the mass equation scare anyone or I'll be explaining each one okay so like hopefully I try to make the mass equations as like reasonable as possible um in theory if you write this down on in py torch um oh yes if you write this down in in pytor you actually have a working implementation of of a transformer architecture um and yeah so like we'll be talking about each component separately as well um yeah is anyone scared for the math no yes no no okay very good okay let me just check questions does anyone have any questions okay okay so did you have a question okay well um so from my understanding from the layer level for Transformers um so it's almost toico in a way is that I mean of course the hard map behind it Cosmic sorry what Cosmic flinko like the drop you know there 80 layers right in a grid right so like low energy something sorry I'm I'm actually not familiar with that D you have to explain to me what did you Cosmic what sorry cosmico Cosmic clanker do I know that I'm not that smart so you have to like explain to me what that is should I search that up Nota like the arcade game of and you drop that oh yes it's a Windows XP output output you know on the bottom right yes visualize it right yes like if you take this to visualize it yes and then and then visualize the map for example let's take llama 3 88 right M it's 32 layers right and then layer zero or I guess doesn't matter there's layer zero and then layer 32 which out is the output layer yes now when you drop a prompt into this Cosmic machine yes which it's not actually Cosmic we just don't understand okay yes it's not Cosmic yes it's just Maps but so far it's Cosmic to us I don't think we understand personally I don't think we understand 50% at the like where I I think we exended a lot but okay yes okay this is mat you're the expert not me you know so okay but um so you're just trying to say an analogy like it's kind of like the game yeah so so people can visualize the mapus like I think so oh yeah I talk about that in the slides but like I think it's more like an analogy I think it's more like you're going through like a ma like it's not a maze I would say it's more like you have like a every single layer has like someone trying to make you change clothes and then each layer does like a fashion designer trying to get you to wear different clothes and each layer the fashion designer doesn't like the previous fashion designer choices and they will change your clothes something like that I think that's more like a Transformer it's like each fashion designer has like views of their own so it's actually quite simar okay yeah I guess you could oh yes yes yes wait it is in Windows XP right is it that I'm I'm Mak confused Windows XP like the game I think I played it before Oh okay anyways okay yeah sorry yeah question oh um so when I put subscript I It generally is well technically everything is a matrix but if you see any summation signs um with sub subscrip I then it generally means rowwise or columnwise um The W like if you see small if it's small small like a small W that generally means Vector um but in general everything that's capital is a Matrix um and why is it a matrix because it's just faster um I mean in theory you could convert this all to vectors but for Speed purposes this should be matrices um yeah any other questions okay next um so why do they put this hello my name is Daniel hi my brother's name is Michael I hope everyone to have tons of fun a Engineers worlds is the best um why did I put this well does anyone noticed any similarities between these sentences um or differences sorry exclamation the end ex okay yes okay yes okay except for the first sentence okay anything else just say random stuff interesting yes okay hello and high are the same thing but kind of different words okay yes semantic embeddings basically this isle okay okay somewhat okay are simar okay yes okay okay well to turn the I didn't have any intention the intention yes go on I mean it's the king plus Queen yes yes for water back yeah yeah I know what you're talking about yeah so like you know it's King minus man plus woman equals Queen that's the I think that's what you're trying to show a little may I mean maybe you don't have that attention but this kind of it shows that okay well Theory I guess you could have just seen the next slide there are just five if you simply just look um the first one if you consider punctuation as combined with the word right like hello comma treat that as one separate component and if you do this ignore all spaces the first one has just five components right what do you think the second one okay I guess you can already have the slides but what do you think the second one has then right anyone already were all the anwers there okay yes I wrote all the answers right 68 seven right 1 2 3 4 5 6 7 eight right 1 2 3 4 5 6 seven right so like if you do this um just assume all punctuation from now on is combined with the word and ignore all spaces um and this we just invented a tokenizer um right so this is very general a random tokenize that we just invented um and each one is has an essentially the reason why we want to do this is because computers doesn't they don't understand words they only understand numbers right so like you have to essentially assign each of these tokens as a number like an ID right so hello is ID zero right hello comma actually is ID Z my is ID one name is id2 and so on right so like you have you have to assign an ID to each of these components if you don't do that then a computer doesn't know what you're actually doing right computers only know numbers so we just invented a tokenizer um I would not suggest you to use this tokenizer but in general it's actually not that bad um because can anyone see any issues with this new tokenizer we just created um what are some issues yes without yes very good so what do you think we should do okay so the point was we included the punctuation for the words and that is not helpful right so like Michael explanation mark or Michael not explanation mark so what would you suggest to fix this interesting so hello and then hello will be one token and then comma itself will be a okay interesting anyone have any other suggestions have yes so the point of the to is to reduce the vocabulary but here you haven't reduced the vocabul yes very good exactly so do you have any suggestions for how to improve oh yes very good yes I haven't heard that in a long time very good yes all natural language stuff um so the idea was to stem the word so essentially you can remove like you know for example like skipping can become skip right so like skipped skipping skip like you know they all kind of the same well I wouldn't say the same thing but like in theory they're the same thing um any other suggestions very good idea right if you lowercase the more then you can reduce a lot of issues um is that a good idea Capital my and small my what do you think is the difference if you do Capital my generally means it's the start of the sentence if you do my with no lower case then it means it's the middle of the sentence so good idea though um actually I think I think um so but is an old okay people still use but um I think there is a lowercase version of but so they essentially lowercase everything um and I think it does okay for like semantics and stuff but it doesn't really do well for de decoder type style so don't lowercase but good idea any other suggestions yeah tokens would yes start build oh you just said the name of the algorithm it's called word piece or BP tokenization um yes exactly so that's actually the correct well I wouldn't say it's the correct way I actually don't like that approach but yes it's the most recognizable and most industry standard approach is to do what you suggested which is to start off with like small little individual characters and then combine them together based on some sort of statistic right you you shouldn't like you know hello like maybe hello is a word that we have to select right so like but but like H like h e l l okay hell okay that might be a popular word as well in like the dictionary but anyways or he right so he hell hello um each of these might have assign different tokens but they might not be right so like it depends um and the correct industry standard is to use these methodologies to build up the component um which we won't be discussing today but like that's for later research um yes okay so let us just look at one sentence right the first one is hello my name is Daniel assuming our tokenization is useful okay let's just assume the tokenization which we created is helpful okay so remember it is just put all the punctuation together um ignore spaces and don't do lowercase or whatever just just you know tokenize it as as it as it is it's not very useful but who cares we're just say this is a good tokenizer um the question now is if I select the first token hello right let's assume I don't know if okay the color is not very good but like my name is Daniel is gray out I I think I'm not sure if you can see that grade out pretend someone types in hello comma right a language model should predict what's the next word right so how does it know to predict my right you know when you type chat GPT it like it goes from left to right like you type something you know you type some sort of instruction and then chbt will like print the words from left to right right so like why is it printing words canone tell me why is chb printing words from left to right why is it not doing right to left or why does it not just spit out everything in one go yes exactly correct so because it's a Transformer type decoder type architecture it is predicting the next word based on the previous words so very good um and so the point is is we only can have the language model see previous words not future words right if you accidentally put the future data in oh you know your accuracy might go to 100% if you use future data so please do not do that this is actually a very common mistake like I'm not saying this jokingly it's a very big issue in research so please read the paper before you actually like if you read research papers please see how they do the methodology um always read did they put the always ask the question did they put future data in when they did the training or doing the research paper okay this is a pervasive problem I'm not joking um you can see like weird accuracy like if you see papers which have like 98% accuracy question question question question question okay or 100% accuracy how's that even possible how can something be 100% accurate question right and so the most likely scenario is they use future data yes the paper itself will actually catch this or I feel like I often find that the logic in the paper makes a lot of sense finder that is a very good question so that the question was like is it the paper the research paper like the methodology itself using future data or is it the code implementation the problem now that's actually very good question because it depends so I think if the researchers I think normally in research papers one person does the coding some some other people do like secondary coding and in the main researcher makes the idea they don't actually do the coding so the the the coder might have misinterpreted the methodology to be honest I don't think so that's the case I think it's actually methodology that's the problem I think if the researcher like the main researcher if they find out that their method has 9 % 98% accuracy like why didn't they question the results like I would like wouldn't that be very sketchy like 98% accuracy 100% accuracy I'm not joking this is actually very serious problem so if you read enough research papers you'll see this problem is always the same problem it's always using future data um so yeah I think that yeah I think like if you don't yeah okay w't comment in any papers anyways uh yes okay does that kind of answer your question so it depends um I think in general I would say it's the it is researchers the main resech the lead researchers responsibility to correct these mistakes um then you shouldn't be the re lead researcher so like that's my take on that um I think the coder I I mean the programmer might have some issues but I don't know I I think I I blame the lead researcher um any other points yes if you see huge leap in performance question question question it's probably future data actually I wouldn't say it's probably it's like 50% sure it's future data um yes question oh yes um exactly so the point was um in the test and train split sometimes although you might not be using future data you might like accidentally put different components in the like the training set from the test set and it depending how you split the data set you might actually mix the data sets is that kind of correct basally yes the distribution itself yes yes so that is why you should you have to be very careful to how you split the data sets right for training and testing you have to use stratification you have to inspect the data before you do it you must enable random shuffling right there is actually a very very common question on pytorch how do you not enable random for for hugging face how do you not enable random shuffling for hugging face um they purposefully disabled this like think about like why because like people might forget to randomly Shuffle um and actually this is a pervasive problem with Calo competitions people like to like train on the tests like they give you like 20 normally speaking you should have 20 test 0 train right so like but people like to like the final submission they use even the test in the submission so you know um it is a pervasive problem um any other points and questions okay so what is a language model given the word hello can you somehow predict my name is Daniel right so like given the word right so given that token can you predict these extra tokens um remember carefully I purposefully only went from left to right right so like Hello can only predict my can only predict name is and Daniel right it cannot predict right you can't use the word hello to predict previous words right that's cheating so that is sequence modeling and the point is when you chain them together let's assume that you predicted the word my as the next word right so now you have two pieces of information the text that you see is hello using these two pieces ofation you want to predict name is Daniel and you keep doing this and that is a language model right that is essentially a language model what is it doing is you start from the first word you predict the words into the future um and you keep doing this iteratively right and so that is what CHT is doing um does does that kind of make sense um and remember the point is never use future data okay I'm I'm I know I keep stressing this but like this is actually a very big issue in machine learning and AI right this is actually the biggest issues I find in my like opinion is using future data there's so many papers which do this um yeah so the second point is you must tokenize each component into numbers remember this so I'm just going to you know cook up some numbers hello will be 0.11 minus 0.123 102 okay just made those numbers up um Daniel is 0.11 123 minus 0.122 okay I just randomly made them up um and remember you must each component must have the same number of numbers right so like if Hollow has three Daniel must also have three can someone tell me how many combinations of if you assign this case that each number must have three um you know three numbers how many combinations do you think there can be for each token or for each component how many combinations what do you think the answer is so remember you can choose any single number in the three numbers right 0.11 0.112 0.111 1113 whatever number how many combinations are possible dep okay let's assume it's infinite Precision correct the answer is infinity you can do as many as you like um but normally speaking you should use not three right now the question is why should you use one number then right howo can be 0.11 if it's already Infinity 0.11 if you use two numbers isn't also Infinity right so what's what's the problem well please don't you should use as many numbers as possible try your best to use more numbers it's because the computer you know it's not an infinite machine so please use like more numbers so you can like learn which numbers to like assign it to um and when you start training a language model these numbers will be randomly initialized right so like all these numbers will be randomly initialized and can someone notice my initialization what is the issue with this there are like some there is a glaring issue quite obvious um very big problematic issue sorry sorry wait you want number okay same number okay okay good point yes did someone say something what was the other point yes the magnitude is most important right 123 and 102 are terrible um when you randomly initialize please do not initialize with random large components um this will destroy your training that is why your training might have Infinities and sometimes the training loss goes to zero okay that is not that does not mean your model learned anything that just means it's some sort of error in your training data or your initialization so be careful of that um don't worry Huggy face does this automatically for you so you don't need wor um and now each component has a list of numbers that is it It's associated with okay I just use the same number for now it's easier for me to you know do the slides um but essentially hello comma my name is Daniel each of them has numbers associated with them and this is the thing that you're trying to learn for each of these components and remember this can be converted into a table of numbers right so like if you replace all the commas with just column columns right these are just tables of numbers um and this table is what you need to train and again remember given the word hello you want to predict my name is Daniel right so like essentially given that you know Vector of numbers can you predict the other vectors of numbers um yes oh yeah you can do as many numbers so you have to select a option how many numbers you want to select to represent these numbers so for example you can select six numbers or you can select 1,24 numbers or 248 it depends on the model Creator's Choice um yeah yes every single row must be well it doesn't have to okay not it might not be the case that might be not but it should be yeah with high probability it will be unique um yeah wait is it like okay um and so when you do training of a language model um there's a trick that you use and remember we want to predict the next word right so hello you want to predict my right so can someone notice any pattern with this like why did I do the arrow and what's the pattern that can anyone see this any special special like things with this no what happens if you take hello my name is Daniel and just shift it up by one place is it right if you shift it up by one place hello is now aligned with my my is now aligned with name and so on right and there'll be a gap at the very bottom right so we simply we just put EOS which means end of sentence token that just means it's the end of the sentence and we just you know put it there because it's a gap right so remember machines machines do not like gaps and you must use all numbers so that's the reason why we did that and this this is kind of the training mechanism right so we essentially we have that list of you know list of words and we want to predict the shifted words and the Transformer all it does is there a function to predict that so given hello can you predict my given my can you predict name and so on right that's the transform architecture um it's kind of a bit wrong but like something like that okay like the FX is like this gigantic model that can be like you know there's lots of turning in it um okay let's check if there's any questions okay and the point is remember remember the point is we can only use predict the future words right so hello can only predict my name is Daniel and so on right that's the purple the purple component and the blue box is called the attention mechanism the f ofx i factored it out and that is called the multi-layer perception or MLP layer right so there is actually two components in a language model one does the prediction of the next word and the FFX which is the MLP just does this you know changing component um it just makes it like you know a bit better than just simple predicting the next word the attention oh okay there is a question um oh I didn't see okay well um yes I know we are focusing the most approach one but you've got now yes yes the facebook com I don't so the comment was normal Transformers predict only one token at a time how about Transformers which predict multiple tokens at a time yes so actually you can predict multiple tokens at a time it just depends on what is your training objective at the very last layer you don't have to remember we shifted by one place right why don't you shift by two places and three places then you'll be predicting two tokens in the future three tokens in to the Future exactly so before it's just one objective so like one column of the output we just add more um and yes you could do multiple tokens um was it the it was the Facebook Paper right I can't remember yeah it was a Facebook Paper um I forgot the accuracy though um yeah but yeah you could do that um I guess like I I don't see any I I guess it's just good for inference time like you can predict you know you can make inference if you do four tokens in the future you can predict four times faster um man to okay yeah I think it's mainly for when you do inference you can do this if you do two tokens in the future you can have like two tokens in one go um I don't I I don't like that approach I think predicting one token is better um because you're already forcing the language model to do so much you're making even more problematic so I think predicting one to I may for inference time it could work um yeah yes question um for the so token yeah tokenizer converts to ID so like yeah so so the token tokenizer when it has like when it says like it has 32,000 words in the tokenizer essentially it's an ID from zero to 31999 right so like hello we have ID 2557 so then what you do is you take 2557 go to the hash table which has this which has the um this table right I made the table right so hello has an ID my has an ID name has an ID and so on and you essentially you hash you go to that specific row and then you that is your like sample in your training data and you do this like for the whole thing so the tokenizer does you do get a number it's an integer um and you just have to Hash it to the you know the embedding Matrix and you'll get like a vector of numbers if that does that kind of answer your question oh you're talking about the embedding Dimension so if you see 2,5 40 or whatever those numbers um that is just how many how many like columns how many numbers you want to represent for each um component yeah okay any other questions yeah yes oh okay okay okay yeah yeah you had a 496 I guess it just depends like if you make so the point so remember we said how many combinations can we do Infinity because we can do right because floating representation um in theory if you have infinite position it can be Infinity but someone mentioned how depending on the Precision of your float it's actually limited Precision right so like the point is you want your model you want the training objective to actually learn give it as much Freedom as you like right so like if you're trying to restrict the model when it learns it might actually not be helpful and that is why normally people have like these large numbers like you know 6144 embedding Dimension or 8,192 embedding dimens right the more numbers you give it to the model it just more freedom to move um I in theory I I mean in theory it should have better accuracy um in theory everything's in theory um I don't know if there are research papers would show this I think someone should write a research paper on that you know each embedding Dimension test you know do three trillion tokens okay that's probably too many and then see which one has I'm assuming the more you add the the higher accuracy yes I mean I agree on that I think someone needs to do a research paper yes that should be a new research top I I've never seen this paper before so like that would be very interesting so yes question yes the reason for that is it just makes training faster sometimes so depending on so there is I think Andre was the one who tweeted about depending if you pad the token if you pad the vocabulary to a specific number you can actually make training faster because in the Nvidia gpus when you use tentacles if you pad it correctly it can get the data and cach it more appropriately so like for example the cach size is like 64 I think it's 64 okay maybe I'm making stuff up but like essentially you have to pad it to a multiple of 64 something like that and so sometimes that happens another one is like some people want to add when you want to do more fine-tuning when you want to like train the model for more you want to use one of those unused tokens for your own purpose and so they left some holes in there um yeah does that kind of answer your question so when you do tokenization assuming you don't encounter these Hogans you won't have any problems but if you do then there are problems yes um so for example if you do llama 3 fine tuning if you use the base model for llama 3 and you accidentally use one of those tokens you will get n for um for fine tuning right so you have to be very very careful and so like I think what we did is for unso we actually we actually find these untrained tokens first set them to the mean of all the embeddings and you won't have these issues um so I think that's actually a model Creator problem is like they probably should have not set it to zero um I don't know why they did that but anyways um yeah they should have set it the like you know normal distribution or like some you know just random initialization um yes kind of yeah okay any other questions okay oh yes oh yeah yeah you can put a beginning of sentence token I just didn't do that um you should put a beginning of most language models would put a beginning s like what is you know I put the end of sentence you should probably put a beginning of sentence as well that actually very important as well um most models do that now um they found that to be very helpful to be honest I don't think so it's actually that effective I think the beginning of so the BOS token came from the old style um um the old style the CLS token for um I think it was the first token for like bir style um so like they had a classifier token at the very start I think it was at the very start I'm not 100% sure but I think that's where it came from the big I I don't think so the beginning of sentence to can actually makes that much difference um but you should put it you should put it you know giving the model more freedom to move is always better um yes I probably should have put beginning of sentence but you know yeah for demonstration I did not do that um yes okay we did that right so like the green one um right so like the attention block is kind of encoding the stuffff that we described right predicting the next word based on the previous words right and so like the attention block is that first part the MLP block is just the mixing pump component um and this is kind of the transform architecture kind of like in visualized and you just repeat this L times um that is a Transformer um now the another question I always have is why is training language models not over cubed because like aren't you like given the word hello you're predicting predicting my right and now we have hello my you're predicting name and then you have hello my name and you're predicting is and so on right shouldn't this be the training data why is the training data just hello my my name name is is Daniel Daniel right this is the training data that you actually see why is it not this can does anyone know why sorry the complexity yeah yes the complex yeah very bad complexity right so like if the sentence is like one okay if the sentence is like 100 words what do you think how many quite bad yes Bas of one yeah so like 1 plus 2+ 3+ 4 plus 5 all the way to plus 100 right so like n ID two 1 + 100 I think I can't remember my mths but yeah something like that so it ends very bad um and that's if you have one sentence if you have if you have like 10 sentences oh my yeah but like do does anyone know why language models don't need to do this like we don't actually need to do this right so like we can skip essentially instead of instead of having this as the training data your training data is simply my name is Daniel and and shift it by one up and that's your training data why is it not this oh yes we haven't talked about position in codies yet yeah okay but you actually you don't need position en codings oh okay yeah attention what oh yeah the attention mechanism yeah because of oh correct that's the answer yes it's because of attention what actually specifically um mask attention right so that's that's a trick okay we'll be talking about that um we'll be talking about a few times um and I'll give you the code again well actually the math formulas for Transformer architecture right so like a attention block um we will be now talking about the attention block right so like the Z is equal to the softmax of qk transpose over root h plus m v um and as as you mentioned it is the attention mechanism which allows us to skip the o n cubed complexity and make it o n squ why because remember we want to mask out future tokens because we don't want to predict on future data right so like by using this mask weirdly this mask allows you to train more efficiently um you know it's funny because like attenion is O squ so the longer your SE is the worse the complexity but actually there is a special trick which you use a mask and this actually makes attention not that bad um so instead of doing hello to predict my and so on so on so on the attention mask acts as this methodology right so the attention mask itself acts as um you don't need to do like all the complicated you know all of the words predict the next word um okay this is okay probably should have um so we'll be now talking just about the attention itself right so like softmax qk transfers over root DV um just a reminder that whenever you see qk transfers a query and keys um I do not like to like there's actually explanations like what is a query what is a key I do not like that actually approach I would like this to be a maths approach um so my view is given the Matrix X which is your embeddings right so remember hello is a vector of numbers right you multiply this by some weights WQ WK and and WV and you get back qkv Q is query okay keys and values but that's a very vague interpretation I don't really believe like I don't really trust those interpretations it's not that clear um just assume it's just maths okay just like get your X Matrix and multiply by weights and you get some extra weights that's my view um and so that is kind of so like if you see why stor like this does anyone know why stacked it like this like why did a presenter like this specifically why is it like the presentation like this any any points comp sorry what comp composition composition decomposition interesting okay that's a very interesting point um but no correct I just yes that's correct so I just lined it up such that it's easier to see and if you take the Matrix X and you multiply by W WQ you will get Q right and this is actually the correct maths um Di and so I I like people visualize Transformers as maths it's in my view it's EAS okay I'm not sure for other people but my view is easier I do not like it when they say oh queries and like you're trying to do keys and values I don't know what that even means anyways um and the yellow components are the ones you want to train X is what you want to train WQ is what you want to train WK and WV and qkv are just the components afterwards when you have the so remember you have the Q you have the K all you need to do is when you do K transpose you transpose The Matrix and you do Q times K transpose and you get this big Square Matrix called qk transpose right hello my name is Daniel and so on right so like that that's kind of what I want to visualize is like you know it's actually a when you do Q times K transpose you get a square Matrix um and all you need to do now is do the soft Max divid by d right so soft Max essentially each row you normalize to one right the sum of the exponentials must be right you need to like normalize them do doesn't know why you should do that and why should you use softmax any clues why do yes yes okay that's the answer yes but like why like why sorry when you multiply them you can get nans oh yes very good oh that's okay do you know how to fix that close you have to minus the maximum of the row that's how you fix it um yes oh yes very good okay yes we want to sample from that okay sample from that distribution but what happens if you don't do the soft Max doesn't this still work or not like what happens if you just do qk transpose over root D remove the soft Max like why do I have to do soft Max yes interesting that you can fix that with like minus Max of the Rope as well with exploding anyone else okay what happens if you don't have a nonlinearity then so it does it have to be softmax can it be something else could something else yes it could be yes that is another active era of research which people should focus on which is like why do we need to use softmax um generally speaking research papers show that is actually the most accurate um if you use other activation functions it might actually not be that accurate right so like um but this also is the bottleneck of Transformers is because it's a softmax it does the row sum of the exponentials um this means that you can't actually decompose this right you can't actually bring the Matrix Matrix multiplications out um and so if someone can find ways to make this faster you know you get like millions of um okay maybe like much more than that but um yes and V is just remember the V comes from here right so we just take the V multiply it up again and we get this Matrix at the very end and that is right yeah that that is the final component right this this empty box is what you get out from the attention mechanism for the layer Norms um I don't really want to explain too much but the lorms essentially you take the um you take you square all the element El per row you sum them you divide them by the square root and you take the mean and they just do one divided by right all this does is just normalizes the row to make it easier for the language model to learn right so like why do people do lorm it just makes training easier um it's more stable there's no other like there's no other like point there are like some theories like you know batch normalization like you know um you out of distribution you want to make like shift towards the distribution for out of distribution data I just like to think of this as a optimization method um LMS just make it training easier and more stable um and lay Norm is simply remember as I said is you take the X Matrix you do a row sum of all the squares and you take the mean and then you just divide it and then you multiply by some weights it's a vector of weights and that's just lorm um you don't worry too much about like what is lorm or what it does it just does training better more stable um please add as many layer Norms as possible um yes add everywhere lay Norms everywhere and you'll make training much better um okay I probably okay I don't know if you can see this but in Triton right in order to write Triton code for the Lor this is the forward kernel um we will not be talking about Tron today but um it's actually not that complicated if you read more intensely um ignore all of the like components there was only very few lines for the Lor um it's actually not that complicated um the rest is simply just how to load the data um it's actually not that hard um yeah the backward kernel is when the problem comes um how do we actually do the differentiation of the layer Norms right remember you have to train the W right is yellow um you actually have to train that um how do we find the derivatives of the W um it is very complicated um and if you want to learn in your own time you can have fun learning the derivatives um it is extremely complicated because there is like sums there is like um you know row sums how do we do the derivative of a row sum um it get can get quite complex I wanted to talk about back propagation today but I thought like it's probably two heavy Maps um so no back propagation but we'll be showing but I do have um tutorials on that so if you go to the Triton tutorial um I followed that that's actually quite helpful um and the backward kernel is just very very problem um now up to the Rope embeddings why do we do rope embeddings does anybody know what is the Rope embedding yes it's it's a way to extend so you could use the rope embeddings to extend context yes do you know how how does it extend context so well how does it work yarn or like or how would you use rip edding to extend context how what would you do how would I how would I do that I would create a basically what I would do is create kind of a you just multiply the base by two and then you get two times longer context you multiply the base by 10 problem is looking for like one million cont right then the model a part of is trained at like 4 correct 4K right yes so that's where yarn might kick in inad of like so is that the dynamic dynamic well sta or Dynam either way so how would you solve the problem if it's like you want to train if you want to have 1 million context L but your data set is only 1,000 words how would you solve that problem how would you think of solving that problem because like um some people have said they do 10 million contact L is there any data sets which is 10 million tokens um how would you trillion tokens oh no no but that that that's 15 trillion tokens for like the data set I mean like how do we do long context remember when you do long context training you have to have a document which is at least 10 million words for to learn how to predict the 10 m million plus one token so um how I would solve the problem would just be to gather better and more diverse data set yes that's that's the that's the ideal so what happens if there is a DAT there is no data set which is 100 million tokens then what would you do if there is no million tokens I would C I would I would synthesize I how would you synthesize if the model it's like a chicken and egg problem huh how would you do synthesis so no no no I would I would basically um just create I would basically use like CLA or like any of the state of the art models with like Laura and then get and then basically turate but are they trained on 10 million tokens huh if the model itself wasn't trained on 10 million tokens does it do so if I was to try to solve this problem for like client for example like let's say they code bases in million tokens or you know and they want a 10 million something contact or whatever right then I would um basically like create a like synthetic data set not synthetic but a derived data set from what we have okay interes assuming we do not have but I can't assume that we have no data right so good point okay I don't know I I think it remains to be seen like many claims by companies 10 million contacts 100 million contacts I question question question question question well I've only seen one million actually work so I mean yeah and that's bring detention right okay okay now we're going into okay yes okay okay no no no that's fine I was asking the questions but okay wait the question was like what is a rope and betting um someone did mention like positions what does that actually entail what do you think is a point of Reverb embedding all it does is you want to tell the model to learn what is the position of the words right so like hello my name is Daniel it actually has a meaning like hello is like the first token right but then if you put hello as a third token what's the difference there is a difference right so like depending on where the word is in the sentence it matters so the whole point of embeddings rope embeddings is it learn it tells your model to learn where is the position of this component um and old style they use um absolute like relative like you know absolute positions um rope eddings does like some special tricks like you know times the cosine plus the um times a sign and does some sort like special rotation and stuff like that um the paper found that if you do rope embeddings it actually has high accuracy um and you know everyone does rope embeddings now um yeah you mean lower sorry the position the very beginning mention is lower yes there is I think bird did not I don't know did B use rope I don't think so B use absolute yes that's the problem I think B use absolute I think um I don't remember anymore but oh yes yes exactly so rope did not exist yeah and so like this paper the r for paper shows so previously people use absolute position encodings which simply just adds a position like you can literally just add like if the position is one is zero just add zero if the position is one add one if the position is two just add two that's that's literally what they do well actually well not not exactly but like you know what I mean right you have to divide it by some sort of normalizing factor right if the position is 30,000 don't add 30,000 right you would like destroy training but that's kind of what they do um and what they show is if you do rope you can essentially increase accuracy somehow um and we just use this as R we just treat this as true and everyone uses rope now um yeah yeah in that case do you have an opinion on yarn versus Rus Yar so yarn is kind of rope so yarn just does I I'm assuming yarn is is rope but it does it does like actually I I don't think actually commment at this because I'm not an expert on that doesn't it does like it does yeah not so I'm no expert on obviously I'm not expert on one context r y but since BLM only supports static yarn unless you're planning on trying to go to one million context it's actually kind of terrible is Yan the one which it does like the base like randomly changes like if you have position if you have like up to 1 million context and you do 1 million and one context the Bas changes with that like the factor changes is that y Dynamic changing yeah that's the issue that shot right that's not a short P shot right that's not anywhere close to one million conts in theory or maybe like let's say a three shot or two shot um like so did and but obviously Dynamic yarn in theory could fix this rope issue where like we just take this rope as gospel um but are you following kind of my question here yes I no I think Dynamic ywn is just Rope though like okay that's weird can maybe I unplug and replug so like the screen kind of went away let me just read this do this again is this like is it is it is it not not working or is it like screen or no screen that's weird can you answer question oh yes okay yes I'll answer some questions um yeah anyone anyone else have questions wait okay wait I need to refresh the oh if anyone has like take a break you can take a break now if you want um and if you have like other questions yes okay question sorry sorry Yan is I was so so the question is for contact Windows oh okay what is the Improvement okay so so the point of Y like what we were talking about is like how do we make a language model learn how do we make it do long context without training on Long context kind of yeah and so like what Yan does is you can essentially extend the context window automatically by dividing the base of the Rope embeddings you change a scale factor um I I don't actually have slides for this but um it's just a methodology which allows you to scale the factor and you essentially magically make the model learn new context long context um that's kind of what Yas yes it's extremely if you do 1 million contacts L then your o squ is 1 million squ which is horrible um there are like some other methodologies like you know they want to do like linear Transformers and like you know um yes I guess you could try that but I don't suggest that um yeah so hope yeah yeah sorry question very new comment U for time tuning versus in context learning from low what is the difference between in context learning and fine tuning or what's benefit how that is a good question so fine tuning changes the grade uh you have gradients for fine tuning so up yeah I'm interested explain how it works my your opinion what what would you do when I think it depends I think I would do in context learning first so if you have like few sh prompts you shove it in to see if it works but I would still resort to fine tuning if want to be more efficient and if your model doesn't seem to be learning then you have to go to back to find tuning there was a page which was released yesterday I think was it yesterday um it showed that in context learning is very useful um and it like learns kind of like how to be like a random forest or like a tree I I I think it was yesterday the PayPal that was quite useful um very interesting um but I think fine tuning is still very important especially if your model is not learning anything and it doesn't seem to be working then you have to like use fine tuning to change your behavior um I don't really have a comment on this like I just feel like you should do everything um and try everything um yeah any other questions well I I have a couple app I don't know oh did you okay I think my app kind of glitched you want me to read the one one oh maybe I'll okay wait yeah wait I'm sorry I okay maybe okay let's just I'll continue and then we will do the questions yes yeah okay I probably have to okay I don't okay I did not time been speaking to okay anyways that is the Rope kernel um and it might look horrifying it's actually not that bad um it's literally just the formula that did Q * cosine plus Q time a rotation Matrix Time s and that is just rope it's actually not that hard um just the code is a bit more annoying but it's just like moving the data it's just data moving and data moving data and stuff like that all of the code is just related to data movement so not that complicated um the most complicated part is the derivatives for rope um and you have to use something called rotate half which essentially rotates half of the so okay just read the code it's like minus X2 concatenated with X1 so you're like you essentially take the Matrix X you divide it by two you take the first half you put on the second half and you put you switch the ordering and you the first half becomes minus and the code generally is reasonably well hopefully um for Banning but the question is and so like this is hugging face code right so like Q * cos plus rotate half q * the question is how do we actually take find the derivatives of this um this was actually very a very complicated phenomenon because I could see many implementations not doing this correctly um and it is very special the derivative um simply if you notice is rotate half the function is literally a matrix multiplication right it's Q * R where R is is a rotation Matrix and the rotation Matrix is minus identity and identity um and Zer is on the diagonal um and if you do this if because it's a matrix notation now simply the derivative is the transpose and so if you do the transpose the minus sign just flips um and if the minus sign flips it's literally the same as your previous example just put the minus um okay probably like explain this too quickly um but the point is if you do Matrix multip application you can derive derivatives very simply my suggestion is shove the derivatives in wolf from AFA or like you know your favorite tool you can use chat GPT as well and you will get the derivatives back but you must put it in the form that's useful for the computer to see um now we talking about the MLP component right so we completed the attention we completed the Rope the layer Norms the MLP is just a mixing component right so it's an activation function time time weights you know multiply some other weights and stuff like that um all it does is it mixes the signals to make it like you know more fancy um you do in theory you don't actually need the MLP component um like most attention you don't actually need this part but you must put it for the model to have more freedom to learn um and you know the famous paper um glue variants improved Transformer um you know very famous author I'm assuming most people know him um but he showed that um if you add glue um SG glue and all these other variants um you can actually increase accuracy um once again this is just treated as fact in the machine Learning Community um we should be doing more experiments um than just using this methodology but you we just treat it as fact and this is in Transformer Plus+ um the architecture which everyone uses um there is a very big difference though um in gbd 2 in gpd2 they don't use the glue variance um they simply just use a normal MLP right so x times the weights up do some sort of activation function and then you down project it so the weights down right so that's but then if you do SG glue and these new variants the glue variants you essentially add this component where you do element wise multiplication um and then you do a gate a down projection so it's actually very similar to the gbt2 architecture you just add an extra component um and so I try to like there's also like a naming change like up and down um up and gate and up like you know changes and stuff like that but in general it's you can see it's very similar um just the extra element wise multiplication component um yeah but there is like a new um the newron paper for example oh sorry the newron the new model by Nvidia did not use glue and instead they Ed squared value right and they show that you don't actually have to do glue anymore you can just use squared value and it seems to do okay um although it remains to be seen if actually is good but yeah so like they showed that if you do squ value you can remove this you can essentially go back to the gpd2 architecture right you don't need to use llama architecture anymore so llama and mistal Gemma all used the second equation gpd2 use the first equation you can go back to the first equation but the trick is the F must be very special and that's called squar reg value and they show that if you do this your accuracy does not degrade that much um and so yes the paypa and interestingly if you see one of the offers it's the same offer one of the office names is similar um you know the name is the same so you know they were also the ones to Showcase a squar value you don't need to do glue anymore um and you can simply just use squar value as well there is a research paper which I highly suggest people to read and it's called the physics of language bottles um it is extremely long though um but it has many nuggets inside and I highly suggest people to read this um they show example they actually did so many testing tests and experiments and they showed that if you used glue variants or gated MLP it actually reduces the models capacity to learn on small models okay so that's that's the point it's on small models on small models on small models if you do gpd2 the first Formula it does better than if you do llama M or Gemma the second formula only on small models right that's the point only on small models um and there are other special things inside the paper which I highly suggest um it's extremely useful for example they say that if you do if you change if you don't if like for example the activation function which activation function did you use um should you use S with jell or r or whatever that's not that important right so like it's not not that important if you use like biases oh it's not that important um so like there's so many different things that you don't need to do um the paper shows but you know people just treat it as gospel oh we have to use this specific component um my suggestion is you know we should do more Tes in the AI space like you know which Varan are the most important but I think this paper is pretty useful um the code for like the you know the swigle kernel is that's the forward kernels again it's not that complicated to like the second part is actually useless it's the swigle kernel is literally three lines it's just it's just the three lines which are commented the rest is just data loading how do actually load the data into the GPU all it's not that important um and you know if you use Tor to compile now you can simply generate these kernels automatically um and this makes your training much faster so that's what I suggest people to do um just use Tor to compile you don't have to re rewrite Trion kernels yes sorry w w k oh gate oh it's just another Matrix so w gate W up you train this um and W down these are all you train them W down W up these are all you train they're just numbers um so like matrices x * W gate is like it's so remember the um uh this thing right W this is for attention W uh x * WQ w k WV assume it's just w gate W up and W down um and it's the same thing so you just train these um does that kind of answer your question or oh no no no it's just a naming convention W up is just it's just a it's just it's actually called up production the naming convention is like WB w a WC it's just a naming convention that people like to use W up W gate and W down um they have meanings so w down is a down projection up means up projection um so essentially you take the Matrix and you like make it larger and then you project back to a smaller version it just makes the model better to like know make the model has have capacity to learn um so it's just a naming thing um any other questions okay um the as I said like before the derivatives are always a pain if you do the derivatives of s Glo it is a nightmare to do and I do not suggest you to do this um but I had to manually do all the you can see all my comments so like the comments are actually there if you do see more carefully I wrote it in math formulas of how to actually take the derivatives and it's extremely painful um I highly suggest you not to inflict pain on yourself by doing this um it took me many days to do so do not I don't suggest this um yes question that is a very good question so I use Desmos so desm is a graphing graphing online graphing calculator you type all these equations in and then you can see does a graph align um oh every single component you have to be careful so every single component has you have to check um oh is this component correct is this component correct check all of them um and so like I normally so like you isolate each component separately and then test it I I'll talk about that actually yeah um any other questions no yes okay um and this is the cross entry loss konel I'll probably just skip this don't have enough time so um I wrote this as if you want to inspect the formulas and stuff like that how do we do the derivatives for this um you can do this it's not that complicated um actually it is very complicated um I did spend a lot of time trying to like work out the derivatives um uh it might be a bit foreign for some people for the derivatives the main reason why it get get complicated is when there's sums right for like whenever there's sums I just like you know doing derivatives when the sums is always painful um Matrix differential is actually very easy if you do x * W the derivative of w is just X transpose it's very simple but if you do derivatives when the sums uh it's horrible um yeah it's quite horrible what you can do for the sum when you do derivatives if you transform the sum into a matrix Al matrix multiplication right so sum is just x times like a vector of all ones so that's called the row sum and essentially if you do this you can actually make differentiation much easier um but I will be talking about that um that's for another topic um and someone was talking about stability for softmax if you minus the maxim of the row you can make softmax much more stable um and this is to like reduce exponentials of large numbers and you like essentially it takes over the entire exponential right and so like if you do this trick when you minus the maxim of the row this makes training much more stable um always do this um yeah always do this um yeah and that's the code for the forward um not that important oh yes I wrote the code for the backward as well um oh wa so this is the sorry this is the forward um the forward um and it is quite long but I wrote all of this down for your own leure if you want to read and implement this have fun um but I wrote this step by step right so like take y equal to log sum of X you know then I simplified it out like you know if you if you exponentiate both sides right you can do exponential of the Y is equal to the sum of sum of exponentials and so on right and so like I wrote this all down um for the but there is a methodology which we showed in unso is you can use chunked cross inter and this is actually very helpful for large models um your logits are very large so if you chunk them you can you can make multi-threading much better for the GP you and so like the problem though is like the derivatives the forward propagation you have to be careful now when you do chunking um so like essentially you divide these into sbus and paralyze each component um I also wrote some you know maths and stuff like that for you to review um and how you actually do the chunk sum is very interesting um the chunks the log chunk sum of the log sum exponentials is just a log sum exponentials um there's like you have to do some manipulation and it's actually very interesting um you don't actually need to change that much code to make it work um okay now we go to the next component which is to investigate the Llama architecture um hopefully this works yes question it is part of unso currently um I think I heard that the py toor team will be including this in Toral compile Al I'm not 100% sure um this reduces memory this just makes this makes long large context oh sorry large vocabulary sizes work so the biggest issue why you have to do chunking is Cuda um Nvidia gpus has a limit 65536 I think that's 2 to the^ of 16 or not I think so yeah yeah so there is a limit and so if you go if your vocabulary size is larger than 65536 you must do chunking um yeah you have to do chunking so like if you like I I think was it J 128,000 although I'm getting confused I think it's 128,000 you have to divide it into two um and so like your chunks would be two um so it should be in py torch in the future release maybe um yeah so now we learned all of this now you can read the code for llama so if you go to um if you go to The Code by modeling Lama py um it should be in the slides but you can also type in Google like modeling uh you know modeling py uh this right there is your if you go to line 94 you have your rope edding which we talked about right just assume don't worry oh okay I need to okay I probably use my mouse yeah okay I don't really like how GitHub does the it's kind of annoying sometimes but I think I can disable it if I log in anyways the symbols um so if you go to this right this is not important if you go to the first one is line 74 the Llama RMS Norm right this is the layer Norm kernel right this is the code for the lay kernel just this much um right this is the you take the row you take the sum of the you take the squares of the like each row you sum them you divide it by the mean remember this is the only reason why you do lorm is to make training more stable um and it's actually not that complicated it's just these few lines um the rest as the rest is just bloat code for like you know you have to set stuff up you have to do random initialization blah blah blah comments and stuff like that um but that's the Lor kernel the Rope kernel the rotary embedding is a rope kernel this is just setting stuff up this right this is setting stuff up um forward right this is the most important component which is the forward component now don't get scared by this that's because we fixed a bug so this is actually our bug fix that we did so this is all in Transformer architectures is like you have to be very careful when you downcast to float 16 if you use B flat 16 right this is actually very important before you have to be very very careful um when you use float 16 and B float 16 training mixed position training cuz it were downcast incorrectly and your rope embeddings will be wrong um so it's actually it's not supposed to look as ugly as this but unfortunately it looks ugly now um before it was just this right but this is just setting up code um that you know we have to like fix the bug and stuff like that um yeah so it's actually not that long right so like it's it's just this um yeah up to here so whenever you see these architectures it just looks complicated but it's yeah no no no so there are like some if you do torche compile we're still like I think 30 30% faster so no matter sorry if you use Tor compile plus unso oh it's with store 30 times faster oh not 30 30% so we're two times faster than hugging face plus F attention 2 but tors compile you know they're adding kernel from multiple packages in so yes they could learn from unso and put it in I'm assuming they're doing that I did have a talk with them so yes they're probably doing that already but yes we're still 30% faster um there was like some I think someone tested this last week um yeah is that any other questions okay this is not important this is for this is the we talking about for linear scaling for rope embeddings if you want to extend the context length that's kind of what they do right this is this is for scaling not not that there is a rotate half rotate half part which I was talking about right how do we actually derive the formulas for the gradient uh how do we actually do the rope and Bings is just this um the MLP which we talked about again remember the MLP layer right so there's a gauge projection the up projection the down projection um this code is bloat again ignore this it's just this is just one line that's the rest is blo um ignore that right so like the down projection is just the down projection right the activation function the gate projection times the up projection right so that's that is the MLP that is the you know Swig glue one line not all of this and that's just for training purposes um repeat KV is just an that's for like you know um okay won't explain this too much but that's for the attention part um this is just to make inference faster when you do 2K and when you go back to slides um if you go to um where is it right WQ WK and WV instead of training WK and WV you train a small s and you repeat this and this can make inference faster um and so like you don't we don't actually train the full m The Matrix size WK anymore we train a small Slither um and we just repeat this the attention again all of this is just preparing right this is just preparing right okay ignore ignore ignore blah blah blah bloat get rid of this um don't look at that um there is q k and V that's the Matrix part um there is some the only problem I find that people struggle with is there is like Dimension manipulation you have to like manipulated the dimensions of the output that is actually kind of annoying I do agree this is actually very annoying but just assume it is the this right that's all we're trying to do is this um and that is just these these three lines do this do this um and then we want to do qk transpose right oh you have to sorry you have to apply the r betting don't forget to apply the rer betting um and then qk that's repeat KV repeat KV is the one that the trick that I said to make inference faster um by repeating the so if you go back here the K and the V the K and the V you take only small slivers you only train small slivers and you repeat them four times um and it does not reduce accuracy that bad um and that's repeat KB this is the qk transpose right Tor on mball qk transpose this is the attention part soft Max drop out okay no one uses Dropout anymore get rid of that line um matrix multiplication right so this is so up to here up to here is qk transpose over root D softmax time V right so up to here and then we do some sort of and then we have to do some um output projection as well um the rest okay whenever you see this part just ignore that right whenever you see like if self. convict do pre-training TP is more than one this is just for um this is just for faster training so you can get rid of that well not for faster training for like you know training across multiple gpus you don't you can ignore that and just assume it's is just that one line uh now there is more code like flash attention to ignore that's just for f flash attention ignore ignore ignore right no you don't need to see this s um scale do product attention is a faster version of attention that is native to P torch also ignore you don't want to see that as well pretend you didn't see that um and then finally we get to the decoder layout right remember each we show uh okay wait maybe I should exit the slides um where is it no so remember the decod delay um so this right remember we say we repeat L times right this is repeat L times this is just shoved in in the decoder layer right we call this one decoder layer and again we do the layer Norm remember put lay Norms everywhere do laym do attention add some residual um this also training moreable right when you add residual it makes training more stable do more L do more do an MLP add more residual and then we complete it right that's just one that's one component of the decoder and remember we repeat this L times and the rest of the code is just doing this L times now where is L times um comments comments comments forward right you go to forward um yeah in my opinion put it everywhere oh that's just by yes why is the ordering why is the ordering do I think it's lorm first then add residual is that correct or maybe I'm getting confused yes ordering to turn the trip it doesn't really matter I think there are some research papers which show if you switch the ordering it maybe increases like it decreases accuracy by like 0.01% to be honest we need to do more testing again like in my view shs everywhere um this actually this should make training more stable um but it makes training slower that's the problem um that is the only problem but I suggest you to put layer Norms everywhere wherever you see at just shove layer Norms um yeah LMS always work um okay and so why do you add the residual of the previous so you do you take the resid you you save the state before the lorm yeah and then you and then you do a layer norm and then you add back it in so why do we have to do that is that your question yeah oh you can do it before if you want to you could try it it depends I think it's for ACT yeah it depends as I said all of this is like gospel like oh why do we do this it's just people tried it and they said it works um oh what do you mean by not an issue like why does this work or why is this like not a what what do you mean by issue I mean kind yes it scales it yeah it's just Maps you just let it just it just works yeah because like because like if you do that the autograd engine will still know that you did it and the derivatives will still be applied correctly so you just assume that it works um to be honest that is another research topic you should try that um I'm being like serious like all there's so many research questions that are like open like why doesn't everyone just put lorms everywhere like why didn't you put a Leonor after the multi multi-ad detention put a Lon Norm after the swalo put a Lon Norm after the inputs put Aon you know everywhere um and my my hypothesis is to makes training more stable like makes it slow though yes in Mech interpretability not solve explain this do you mean like what do you mean by like explain this meability where you do which method do you mean like doing the auto encod style like Spar just direct weight just looking at the weights and what that happened at INF time or training do you mean like the uh what which method like the um cuz I know there's like many methods for mechanistic interpretability like there's different types of methods this SP Auto encoded one or the well Transformer lens is ne's tool okay I I don't I have sensor I'm not that I'm not expert on that but so but you can see the um the literal like um activations yes yes the activation one yeah yeah so basically what so I mean I'm going to not diers but um so basically can do you think we can figure out why this works this way like because you said it's kind of an open question right but do you think that using tools like Transformer lens where we can look at training activate or not activations but like Steps where in fact we would have to like um I'm not sure if I explain it right correctly but like do you think mechanistic interpretability is a path to understanding this good question um could mechanistic interpretability okay H it depends I think my view is it just lorms if it's specifically on the topic of lorms it just makes training more stable I don't think so has any like meaning like that's my view okay that's fair like I think like the mass equations don't show that it has any meaning I just find it to be just stabilized training um there was like papers like the um uh what was the one um batch normalization I forgot what the term was um yeah there was like a theory which shows that b normalization reduces problems of out of distribution data and stuff like that um oh reduces internal coari shift or something um that was the phrase which yeah um I don't know what that even means but anyways um does anyone know what that means there was like a um there was like a video for that as well the yeah does anyone know what that means or yes what do you mean so layer Norms my view of lay Norms is when you do if you don't do layer Norms if you keep okay let's say you take a number two you multiply by two you get four remember there's 32 layers right if you multiply by two continuously you will get Infinities in the end right because you like go out of scope of the float 32 so what lorm does is it makes your number go back to a good scale so if you do 2 * is four let's divide it by four go back to one right and so now it's one again if you times two it's two again let's divide it by two again go back to one so all Leon Norm does is it makes a scale go back to a good scale like it doesn't your numbers don't like diverge on both sides that's what Leonor kind of does does that kind of okay any other questions okay um so all we remember the decoder style oh wa I think we actually kind of finished reviewing the Lama architecture there's nothing else to do um the decoder right you do this 32 times remember like four decoder layer is in self do layers you do this 32 I think it 32 I can't remember um multiple times um that's the decoder you just do you apply this multiple times you do a layer norm and finally you get your logits where is it your LM head right this outputs the probabilities of which token remember we're trying to predict the next token we output probabilities for each token and that is called the LM um and where's the forward function the forward right there's a forward always with the forward um self you do you go through the model and then you okay remember ignore this right ignore this and you LM head that's just one line one line okay one line and then you do the float now another question people have is like why do you have to do the float um um does anyone know why you have to do you have to upcast a float why any clues have a guess I make this bigger have a guess have a guess have a guess why do we have to upcast to float sorry gradients okay close why why gradients it is related somehow to gradients anyone okay it's for training stability purposes so the soft Max you should always upcast the float 32 because it makes training more stable if you take the derivatives and the gradients if you do not do float 32 you might get nans as well remember the exponential is can be very large so you want to take the float 32 which has larger Precision than float 16 right float 16 is the maximum number is 65536 I think I think it's 65536 right but float 32 the maxim is like some large number to the power of 38 or something 10 to the power of 38 so that's why you have to upcast it to float to float 32 this just makes training more stable right so all of these things that we do tricks it's just to make training stable yes said you're times is that just like oh that's up to you so like if you want to do more parameters you can do 300 times up to you that's just make your model 10 times larger so like when you when you hear like you know llamas yeah so so the weights you train when you take the tokens you go through the architecture and like it changes the it changes the tokens and these tokens keep shifting to like some sort of like New Direction and you keep doing this so but if you do it more times get a larer model the problem is you have to train more weights so each each 32 times has different weights correct each of the each of the iteration but not yeah each there'll be 32 different weights for each layer okay and so like yeah normally people just if you see like this like you know gbd4 what is it like one one something trillion tokens I'm assuming there's more layers larger embedding Dimension larger this larger that more layers um normally speaking the more layers you do the model can learn more um so that's the whole reason why you wanted to add more layers you just want to increase the capacity of the model to learn um again is to make training more stable again um and so this remember the shifting trick that we did in p torch the shifting trick is just this and this that's the shifting trick that's the thing that makes it learn to predict the next token um and then you pass through the loss function the cross entropy loss which we discussed about um and then that's the Llama architecture and that's normal the rest is not useful the rest is yeah so in theory you could write the entire llama architecture in like I think 50 lines or something um the rest is just unnecessary BL right this all of this is 1,600 lines of comments and stuff like that but you know this is for hugging faces implementation I it's highly respected um and this is what you should look at first when you read a new architecture um so we just kind of went through the Lama architecture hopefully you can kind of get a sense of it obviously this if this is your first time reading the source code um it's actually not that hard it's not that complicated you just have to see which components you can ignore right it's not that scary um yep does that kind of get it or you guys kind of get that feel um we're going to do more obviously this is the first one um any questions um no not really other than more tokens um I think they Chang embed they did change some of the numbers like how many how many numbers you want to represent for each number they changed that um large vocabulary they did much larger vocabulary and more tokens other than that I no there's no change at all yeah yes the reason why it's funny I used to work at inidia why shouldn't I be writing Cuda right the reason is I see Cuda as extremely annoying to write and if you want to optimize for just Nvidia Hardware okay go ahead you can do Cuda but my view is like I I don't think so that's going to be forever so as a as like a um as like a safety precaution let's just do Triton right let Triton handle the compil compiling down to Cuda or AMD or whatever Intel or whatever right and try to comp the intermed intermediary if you want to get like 10% faster yes you should do Cuda but it's only 10% right if you do fine tuning two times faster it's already like it's already nearly at the ceiling you can only go so much um and so like if you want to go down the extra mile yes more than happy to welcome you to do that but I do know I do not like it's funny because I used to do Cuda all the time but I don't suggest it um you will get more performance though but I don't suggest it yes question oh sorry yes what never drop could yes you don't yeah so so Triton you write it in Triton then it compiles down to Cuda yeah sorry wait actually it could work the only problem why it doesn't work on AMD is Triton oh I think and X formers actually if so if Triton works on AMD we work if Triton if x formers so Facebook's um flash attention Library if that works in AMD then we work oh funny we work but anyways it depends on those conditions so if if AMD has those to work then yes in theory you can remove X forers and just use scale do product detention so there's only one dependency which is Triton um I think some people have gotten into work so it depends yes I kind of have been answer to that I've trained on a mi3 Instinct with one card withon and it work with AMD so okay I mean if Triton works then yes it just works so just I just have an answer sorry okay good you answered yeah okay yeah but we don't so officially we did not support AMD um but I guess it it works okay that's interesting um yes okay um what's next where's my where is it JMA one uh yes okay so we're now be talking about JMA bugs um specifically jamama so if you go to a blog post I actually we wrote a blog post um about all the issues that we found in Jemma um for example you must add a BOS token there is a typo in the PayPal um yes so we don't just find bugs and you know we have to read the paper first to understand the model um now the problem is sometimes when people release models they don't release papers that is very painful that happens a lot now so please model creators please provide papers um otherwise it gets more complicated um there's also like some other issues um and we have a color notebook which provides all these um so if you open up the link G details um in the remember if you don't have access to these slides it is time url.com unso right um that's the slides um if you open up the collab notebook um this is actually runable in collab please log into your G uh Google account for this to actually work um but we show that this is the log L2 Norm so we check the so this layer number right there's like 18 layers we check every single layer the output of the actual good implementation so the Deep m impementation with the hugging face one with the pie torch one with the other ones and if you do the L2 Norm you find that the error is very high and we what we we showed is that you can actually move the error down by doing multiple changes right so each line you can see there's like multiple lines each line is actually a method that we apply to make it better right so like we finally found that approximately either the blue line I maybe it depends on which you like either the blue line or the black line makes training much better um there anyone notice any interesting things about that this graph anything interesting like do you see the you know the so remember each line is a fix that we did right so like there's many lines and we did a fix and it changes the the error what and we selected the black line to be the final one does anyone have any what is like anything interesting yes so one of them caus a huge jump and that is a rope float 32 fix that we did for all architectures yes and the other ones are less prominent but anything else anything else interesting yes yes fantastic why I do not know and that is a good question and I don't actually know I think it's just language you I have a theory the theory is yeah but unfortunately I can't say everything like I I mean my theory is and there was also a jump as well in the middle um and the blue line you know it starts from very low it goes up very high and everything does this right so like there is this some weird transition boundary um in the Gemma model right and so like I'm just going to guess um my guess is that when you do when you train a Transformer the layer the later layers get harder and harder to train right the earlier layers actually get very easy to train and so this transition boundary is when the model probably was not really trained that well um so I'm going to guess this is just guessing um that maybe the model should have been trained for more data um and the boundary should disappear um this is just my guess um so there is a phenomenon essentially is like more dat the model the the LA the last layers are much harder to train and that's kind of my theory but I I don't I don't think so that's correct but okay yes right last one yeah exactly um so in the end so now the question is like why do we choose the black one then why don't we choose the green the blue line um so that's adding the exact jell that we found so if you add the Rope Fix Plus the exact jell you get the blue line but we in the end decided to do the black line and why do you think that is we did not choose a blue line we should have chose a blue line right but the with final all the fixes that we did so essentially the answer why we did not choose a blue line the blue line should actually have lower error right the reason why we didn't choose that is because there was not just one error there were two errors there was many errors and all of the errors combed together we finally chose the black lines because it matches the original implementation so because remember the trick is you have to match the original implementation of whatever the Gemma models Creator the Gemma model creators did um so you kind like just look for this era maybe like I mean maybe like if someone CH chose different like fixes that we did you can probably get even a lower training loss um I guess you could um but we decided to choose the black line because that's what the original implementation did um any other questions oh I'm talking about the weights so the weights are the ones in so the model weights are the ones training right so the rest you don't actually train it's just the weights itself what are yes so remember the the the goal of a transforma is you want to predict the next word right so the sentence hello my name is Daniel you're trying to predict hello predict my right my predict name and so on you have this data correct like you have just take novels you shove in the novels you already you essentially created data out of thin air and then you change these weights using like back propagation do derivatives and try to like change these weights such that you get the highest accuracy um and this training procedure is called back propagation um and so like I was trying to show you like how do we actually derive the derivatives when you do a back propagation you need to derive the derivatives um just use P torch P torch will do the derivatives for you um and yes but that's does that kind of answer your question or okay yes yes yes on actually has that so you can actually depending on your layer so for now what we do is you're embedding and your final layer you can change different weights uh different learning rates so we found that if you train on if you train the last layer with the embedding weights and the first sorry the embedding weights in the LM head by a factor of 10 smaller the learning rate you can actually have increased accuracy so you so yes you should you should change the you should change the learning rates for each layer um but people don't actually do that I think it's because if you set a learning rate for each layer beforehand you're kind of like it's like there's you're like doing subjective bias so that's why people just set one learning rate for all the layers um but I think in this case I'm just going to guess okay this might be a Transformer this is Transformer General this is not just for Jamma this is for all Transformers maybe I guess lay layerwise learning rate could work I think there are like some papers which do that I think it's called LS I think L does layerwise um I think it's called L layerwise um learning rate um I hopefully that anwers your question um yes other it's a log L2 Norm so it's we take the Deep Mind implementation you code it up correctly then you take the other implementations like pie torch hugging V even Deep Mind own implementations and then you check each layer the output you compare it with the original imp the correct implementation and check what's the error and that's the thing that I graphed um and your goal is you want the error to go go to zero right so like you want it to go all the way to zero so like you know on the bottom and not not like very high um and that's log scale right so the error is not like a small number it's 1,000 right so like every single line every single step you go down is a log difference right it's not it's not like a it's I essentially logged it if you did not log it it would look very bad um but I just logged it um yeah does that okay any other question yeah so let's say if there's an issue in tokenization part fundamental thing or we find some optimization a and you have to change the way you're tokenizing uh would you have to retra your models to incate FES this actually happens a lot very frequently and I think like so like for example tiny llama someone try tiny llama and then training already 80% completed they found a bug for tokenization they're like so it happens very frequently and it depends on what you want to do I think it depends to the model Creator if you already spent millions of dollars maybe just keep just train it with the B and then you release the bugged version but it should still work um hopefully um yeah so in theory let's say open AI would have a lot of difficulty shifting if they found like somebody else found an OP more optimized tokenizer or something like that they would have trouble shifting to that model because they would have to spend like you have to retrain everything correct so just just assume it just leave it if you already spent like billions of dollars I'm probably not a good idea to retrain so even if like a like 2x optimization they would have spend they would have retra and spend yes you have to retrain everything from scratch um but that's why like I think like that's why you should do like small scale experiments you know get like a smaller model train it for less data test it and then see if the accuracy is good and then you scale it up um yeah any other questions okay I will yes so there's a notebook so we show step by step exactly what we did and if you inspect the code okay the Gemma code is now the Gemma code um if you oh okay wait no it's modeling Gemma oh okay maybe I should just go to hugging face itself um wait let me go to you can actually find this um if you copy paste this right you edit the you go to jamama and you go to modeling jamama right this is oh did I not okay let me just okay maybe I typed it wrong did they not oh okay maybe I did two L's my bad I always get confused on that um oh what is this this is interesting okay this is like new um so okay yeah I did not yeah so all of this so we wrote inside the for like you know llama does so we show for example in the code now if you go to like hugging pces code for JMA we wrote I tried to write some comments you know for it to be more clear why we are doing this um and so for example the the lay Norm right you have to be careful to where you upcast and downcast um and we write this in here um where is it I think it's no no no not wait is it no I'm pretty sure I've read it some no it is here yes okay it's a bit unclear I need to um make this bigger okay it's a bit blurry but you can see that depending on the model um in Gemma you have to actually upcast a float 32 everywhere you must use float 32 everywhere because that the original implementation use Flo 32 right so you must always follow the original implementation if you don't follow the original implementation then you will get wrong like you know somewhat worse results um and the problem was other implementations just copied llama and Mistro's code and they did not do this and so we found that you actually have to upcast correctly over here right you have have to upcast immediately um and then you downcast at the very end um and so we wrote A Few comments right llama does X do2 float 16 whilst Gemma is X you know it really like llama does that right but Gemma does this right so there are like small little issues downcasting upcasting another question is like why do we have to do downcasting does anyone know why like why is there always like downcasting upcasting float 32 float 16 float 8 does anyone know why we have to do downcasting casting yes correct it's for faster speed um so do you know how much faster like so float 32 to float 16 what do you think it depends who said two okay good guess why why did you guess to well that's vacity okay okay yes okay float 8 approximately two um actually could be more so float 32 to float 16 is actually not two it's actually much more I think it's five I think or is it six um the reason is because the representation of the float is different right so float 32 I have float floating Point representation Wikipedia I think it's in here somewhere um oh maybe I go to beat float 16 um where is beat float 16 brain float be float beat float yes right so like there it is um oh there's more pictures now oh they edited this I did not okay this is new I didn't see AMD fp24 format or Pixar oh okay they have like weird formats now um this is float 32 right and Flo 32 um the exponent has eight numbers right eight bits and the fraction of a bit has 23 um and when you do Matrix when you do matrix multiplication does anyone know how to calculate the number of transistors you need for float 32 does anyone know it's a formula that's related to the exponent the fraction and this just exponent and fraction what do you think the formula is have a guess right I said that it's approximately so if you have B float 16 the fraction is seven right refo 16 has 16 bits you can use exponent the exponent is used for the the dynamic range of the number right so the if you want larger numbers you have to have larger exponents right so this means b36 only has a range of two to the okay it's not 2 to the 8 but like just assume you know it's 2 to the power of 8 okay that's not right but just assume that um 2 to the^ of eight right but is it yeah um and this one float 32 also has 2 to^ 8 there is another format called float 16 which is 2 to^ 5 um and then the fractional component is 10 so all of these numers you can scale right how many do you want for the exponent how many do you want for the fraction you must include the sign bit and the trick is you must have 16 you need to fit you know 16 so you could have like exponent one in fraction could be um 14 that could also work um but does anyone know how many transistors you need to use for float 16 for example and B flat 16 remember I said it's was like around five times faster it's actually not right I think it's even more um what is the formula have a guess how many transistors do you need to use to do float 16 multiplication approximately or float multiplication it's it's a formula related to exponent and fraction the answer is exponent plus fraction squ that's the answer um so what does that mean that means float 16 is 5 + 10 2 right and FL 32 is 8 + 23 S so it is not two times faster it is much faster right so like I don't know what that is what is um so it's 8 + 23 S 5 so you need approximately okay this is approximately you need 537 transistors for float 32 multiplication oh it's just 23 squ so it's 8 plus 23 squ yeah and so what what is the other one I think was um what that I can't remember so um so it's eight and seven right so eight and seven this is this is Google's format it is 57 so what does that mean how many times faster yeah so it's actually 10 times faster right so32 to float 16 B float 16 is around 10 times faster right float 16 is 5 + 10 right so 5 + 10 so B FL 16 is approxim two times faster than float 16 although to the no one really notic any difference um but in general b 16 is actually faster right so that's why it's not two times faster it's 10 times faster um and that's why you must use Tesla t4s as I said because it has TS which does this right the TAC CA does float 16 multiplication very effectively and very efficiently um and so do not use p100s again right p100s do not have this methodology um yes question yes float eight so float eight I don't know um there are two formats for float8 uh oh wait I don't think so it's in Wikipedia float8 oh okay uh floating point there is it's called e em oh it's I just use mini float does it they have some yeah there we go right so you got to decide remember if you want have eight bits you get to decide how many you want to do for the exponent how many you want to do for the fraction or the mantisa part right you get to decide and depending on the company you know it's it's unclear there's no standard um so this one's 143 right so like what's 143 1 + 4 right 4 3 S right is that is that 43 yeah 13 so float 8 is I think it's around yeah so around four times faster than b 16 but in General it's not okay in general it's like 2 to three it's it's not going to be four the reason is because you're packing so many transistors in you also have to do like energy you have to like do the data movement there's like other transistors you have to do I just approximately it's two two to three times faster that's float eight can you go even lower yes why don't we go one one bit so you must have to sign though so you can't do one bit so 1.58 bits some people have been talking about um two bit two bit could be possible the problem with two bit is it's problematic because when you do two bit training um yes okay so let's see let's do two bit right so what do you want to do how many exponent zero remember you have to have a sign bit that's the most important one for the exponent and fraction zero right that's because remember it's squared so plus one oh wait no it's it's Z plus one okay so it's one okay um 10 times faster I don't think so um okay maybe maybe two bit is probably too low maybe four bit four bit could work yeah yes oh that's just because they wanted to do that just for easier calculation like and for their 32 they turn 32 is not 32 it is they have it somewhere Nvidia T of float it's 19 that's the trick they like to like do marketing and they say it's 32 but it's actually 19 um yes that's why it's the same okay any other question was it someone else raise a hand or okay but yes I was going to say like you can do four bit right so forbit is actually a new nvidia's new gpus the um the B 100s do have FL bit um so that is approximately two times faster now the reason it's not okay let me just try 4 bit I think it's 1 plus it's it's probably like 2 plus two or something I don't know six okay right um it's not going to be that much faster because as I said there's power transistors and there's other transistors you can only go so far just the jump from FL 32 to Flo 16 was very large um yes yes so quick question um so for the example the one bit bit that 1.5 bit bit yeah yeah so that would be an example one bit so it's a different so actually I had a tweet about this 1.58 bit and Float 4 is the same in terms of number of transistors you'd rather use float four the reason why is 1.58 bit is you have to do more manipulation to make a work you have to use like the straight through estimator like it's a it's a horrible mess you'd rather just use float 4 um so float 4 and 1.58 bit are like similar you get to create your own base model you do if you replicate if you replicate the paper yes me which most of us have never done right which would be I technium and the Noah research probably reated there some though it does work somewhat somewhat Works um I mean yeah it's one bit but I mean 1.58 yeah it's actually one bit yeah it's I think it's like three call one bit oh they like to call it one bit Yeah but my question is like so in theory like obviously I don't know who works here but like most of us have never built a Bas model yes so like well you could yeah yeah yeah you can with enough GPU power but but one bit bit that you know that was and they even had like a really good great tutorial and like so but do you think that that's just like I'm just asking for your opinion on that I don't think so 1.8 1.58 bit would be the future I think Nvidia is like the focus is on float 4 they might go to float I think float might be the final Precision I don't think you can go any faster with that I think float for is a final no more so we won't be having that much faster gpus I don't think so um I mean float four is actually they don't actually do float four anymore it's like float six for the gradients float six and then float four for the activation like you it's very weird I mean you could do like float three float two but like it's your diminishing returns um so um in arm silicon though uh there's um there's been like advances like like super low fix Point stop is it called Fix Point stop or I think it's called fixed I I knew um has fix Point um oh well yeah so it's uh I mean just like the Snapdragon X like the new yes they have so it's like customizable as well or I I don't know yeah well the okay so the SDK is broken you have to pass the um so this is why you can technically run mixol 8X um 7B on your phone at like 20 something FPS FPS sorry TPS is because you can use ufs 4.0 as flash storage and subsequently use that as memory and then the the but the thing is then you're running at two bit Precision which is um so that's probably why this so if you use two bit Precision that's why you have memory reductions but there is actually papers which show that if you do two bits for the MLP plus four bits for attention that's actually the most appropriate you can actually do that so that so that's not a invalid approach no that's not invalid actually it works it works the mous people did that I think yeah that's yes question sorry okay two kind of related questions on Precision first one is like why is thetive bit like you must have the sign bit yeah you don't have to but it's generally generally like standard practice to have to sign bit in theory you don't have to um the only problem is if you don't have a sign bit your numbers will be 0 one2 right but then what happens if you wanted to like make the model like you're you're trying to not make the model learn negative directions anymore you could do that I don't know if there are papers maybe you should write a paper about that um train a model on that and let's see what okay but yeah related I think all all right yeah uh softmax you're basically just linearly like fitting Stu down there's nothing about that could beon right the reason it's because remember when you do softmax you also have to normalize by the sum of the exponentials and if you do exponential of 10 you already get like some large number and this this probability will take over the entire sum well but you're not you're not likeing it you're just sare rooting it no no it's it's it's the sum of exponentials divided by sorry the exponential divid by the of the exponentials yeah but the BG exponential dominates the right yes that's the problem though if you do that then your model's not learning you're just trying to learn to predict one token why don't you just predict that one token then like the the largest one that you did that's kind of what you you're forcing the model to not learn anything that is why you have to like minus the maximum that that's trick that we showed is like minus the maximum and then you can like reduce this effect of this one token um or this one issue um so it's for training stability purposes um I don't know if that kind of okay probably that didn't answer your question but okay yes that is a good question to be honest I do not know I don't think so it changes too much um lay Norms if you upcast um it's probably yeah small effect small effect but the reason why you need to upcast is because JMA did it before so you have to do it um remember the trick is you must follow what the original implementation does um any other questions or okay there are like some other issues which we showed um that more okay this it's funny because it's all about upcasting downcasting and stuff like that each implementation had does its own thing um unfortunately how do you actually analyze this you have to open three screens up the Deep Mind One the Deep mine one okay okay too excited um you have to open up three implement a Deep Mind One the hugging face one the carass one you have to open three screens and you see line by line what did they do and then now you have to guess which one's the correct one um the guessing part is the most painful so you have to like inquire you ask hugging face which one's the correct one you look at the paper which one's the correct one you assume the Deep Mind one's correct and stuff like that so there's like some human component you have to guess um guessing so that's probably why it can't be automated right these error checking things cannot be automated is because there's a human there which made these made these decisions and so you have to now you have to decide which one which of those decisions did they choose um and you can't really automate this away I guess you could automate this by doing the methodology which we describ try all combinations and see which one has a lower error I guess you could do that but remember you must have the original implementation first um that is a problem um so there's like chicken and egg problems um the Rope position um so this is the one I was talking about upcasting rope um this is an all architectures now you must not downcast rope if you do you will get wrong results so previously on the left right if you see 8192 8192 8192 that's the positions um that is definitely incorrect right what does that mean like do you know why that's incorrect 8192 8192 8192 does anyone know why remember this is positions why is it why is it all the same like does anyone know why this is very bad so we kind of like essentially now we the three words have the same position right 8192 is a position and what is another big error of this there's actually one more error let's assume the maximum is 8192 the sequence length what is 81 92 it's out of bounds remember it's minus one for python right it's 8191 is the correct number right so so if you correct this you get 819 8189 8190 and 8191 right and you can see all the numbers are like this so the point is if you use remember the whole point of this problem is because we're using float 16 for faster training remember float 16 is how much times faster yes around 10 or 5 to 10 something around there right that is why you have to do this and these are the issues prop out because of this issue right we're trying to make training faster but then these issues come um and the jell one which we described before um this was the first bug that we found um actually I think this is the main reason why we were trying to look through bugs is we found that oh look there is this bug in jell um in the activation function and so the point is caras use approximate jell the P torch and the P torch version used exact jel and hugging face also used exact jell um and the question is which one is the correct one is the exact jell correct is the approximate jel correct so what's the difference between exact and jell um jell activation function um there is the where is the I don't know if they have the exact and the um it's called Flex oh okay that's night mode the oh that's even worse okay whatever oh that's prel where is jell oh wait no I have to find it right yes right so like the the exact J is this one right there's an error function um okay my thing is not rendering it properly um but if you essentially what you do is you use Desmos so what I like to do is I use Desmos um Desmos right and literally plot them together plot them on the graph right so like if you have right X Y isal to X over 2 right you literally type this in and what is this one I think you can do error function oh yes you can right you can do error function right X /are < TK of 2 right that's the exact J right now you type in this complicated formula for x / 2 um 1 1 + than what I don't remember this square root of two / what was it pi oh it's Pi Pi and the what um X Plus 0 point can I oh we can't okay 0447 0447 what was the 15 15x to the cubed was it cubed yeah okay right oh is it oh you're right okay wait is that is that oh is it just the rendering problem or is it square root no no no it's square root of two over Pi I think wait is it correct wait something I did something wrong maybe I did something wrong oh whatever who just just assume okay oh wait oh you're right I put the square root everywhere oh is that what you were saying yeah oh okay oh no no no no whoops no get rid of that okay let me just no it's it's tan of everything now I have to do this oh okay probably have to play around of this oh there we go there we go right so the blue line if you remove it the blue line and the red line right they're the same thing but what's the difference remember in I don't know if people know this we can actually do derivatives D DX did anyone know this you could actually do derivatives you get your D over DX and then you can do this as well D over DX right and they generally align right the exact J and the um approximate J generally align and guess what you can also do integration integral of minus infinity oh did I spell wrong oh Infinity to Infinity right of I think this works I'm not 100% sure right you take your exact jelly you minus the difference oh I don't think so this works I I I yeah I don't think so oh yes it works yes it works so what you do is you can take the integral of minus infinity to Infinity so the entire line your minus exact jell and the approximate jell and you do DX and there is a difference right but the difference is very small right it's like 10- 16 it's very very small and notice is like when you do when we do fast Tron kernels I generally use this feature so you can do integration integration and derivatives and you know you can use Desmos so I highly recommend Desmos um and if you do this that's where we found the problem it's like oh okay there is some sort of issue and if you fix it remember the jelly fix does do some effect it does do some effect but remember we only showed there only very small effect so it's not that useful um the Rope fix was the most important right the Rope fix actually caused issues um so you must fix that and that's the most important fix that you must do um and finally there is like some other things that we do um depending on the Precision that you use there is actually a difference between float 16 and bleed float 16 and if you do this we show that float 32 um remember we showed before that in the fixes that we did the lines some sometimes go back up right but actually if you do float 32 it actually does work if you do float 32 position the lines actually don't do separate very well but once you use float 16 the lines then match up again right and B float 16 the lines match up again right so this is just a phenomena that you're using fast smaller precisions and that is why you have this problem but if you do use full Precision you get good results and the fine tuning notebook for the Gemma one also works so Gemma is two times faster uses like I think 60% less memory as well it's more now um so if you run this remember you have to connect to um you have to connect to your Google account and you will get this to run any questions on the Gemma one okay yes okay yes um where did I put the picture oh wait it's in the blog post um yes that's fine um wait where did I put it oh it's the first picture right yeah this one right so the x axis is the layer number so Gemma has 18 layers so each of those the xaxis just indicates which layer the um which layer it is the y axis is log to log to Norm um log log L2 Norm so what you do is you take the original implementation like deep Minds implementation you take hugging face pie torch J like you know implementations you check the output of both of them so the output you run the model through you take output layer one and output layer one the your other implementations and you just find the error um and so this is just the error um and this is log scale um so when it's log scale it looks better when it's not log scale it looks very bad um so does that is that better you're taking theut yes output of each layer yes um that's called Jam so that's for Jamma for fe3 um similar what you do is you open up the fe3 implementation you read through the flee three implementation and because like you guys like prob most likely can go through llama and like just look at it in general remember delete useless parts of the code you will see there are differences in V3 um and the differences are um they use other methodologies um they use upcasting they use stuff but there was a weird thing that we found in the config file um I will show you um V3 config okay just use the instruct version um if you go to always when you go to like new models always read the config file right config.js when you open it up it tells you all the tricks you need to know about the model architecture um and I highly right it tells you what is the EOS token ID 32,000 right when you look at this hm is that a good idea 32,000 32,000 what is the EOS token ID right 32,000 okay that's fine the pad token no h is that a good idea like you have to think about like why these why they there how many layers does P3 have it's 40 right so 40 layers um how many positional encodings does it have so how long what is the context length it is 1 13 1 072 that's the context length um remember it's 100 so this model the V3 medium is 128k right it's not 1 128 right just be careful it's actually 128k right it's 131072 um there are other issues with this model as well um okay this is okay that's the okay that's probably okay probably don't use the instruct version instruct sorry the we CH choose the small version um this is a smaller version there is a thing we noticed is a sliding window so mistro has sliding window um sliding window essentially attends to only 248 tokens um and this just makes training much faster um and does anyone notice what the problem is for this why is it 2047 anyone notice any issues yes well it's not a power of two but correct so is that weird I mean that's horrible yeah so I did ask the hugging face people and they said yes it is a bug so they actually did fix it but then I don't know why they reverted it back so I'm bit confused um they never they kind of forgot about this yeah so it's actually it's supposed to be 2048 um yeah because that that only makes sense because you're training onk like you know to you're training on the correct context right then this sliding window makes no sense in fact I've seen a lot of sliding window bugs recently yeah for some reason yeah I yeah I'm not sure why um but I'm pretty sure this should be 2048 yeah I'm very confident I I'm actually 100% show is 2048 yeah it's not and yeah so the these small issues are they need to fix um they still Haven not fixed um but when unof it's fixed so we actually uploaded models which fix them right so if you go to our unso hugging face repo we actually have models which we fixed all of them oh this is too big um where is the Fe one oh I didn't put it up okay I need to find the Fe one now um where's V oh there V3 mini 4K instruct right if you go to files you go to config.js we fixed it all right and there's other things that we did to fix it um for example the pad token ID is 30 okay that's actually wrong okay okay I need to fix my own okay anyways um there is a bug which we discovered ourselves it should be 30 this is actually wrong another thing is you must not make the pad token the same token ID as EOS never never never never never um this must be a different token to the EOS token um I do not un if we automatically fix this during the loading it's just the config itself is not not right um but that's okay Uno itself is fine um just a config is a bit wrong um oh okay I found my own box but okay yes um so okay I'm not gonna slow down keep going because there's a lot of okay oh yeah yeah actually there okay there's not that much slid okay actually there is oh okay I just noticed more okay um so another one is like fe3 used um they merged the qk and K remember we did qk and V they're unmerged right the weights are separate for the attention mates V3 did a very interesting move is that they fused them into one Matrix and we found that to be very problematic for fine tuning um because if you fuse them together um when you do lower adapters you actually you actually only learn new extra weights and it's very less um so please unfuse them um and we do this so our version of the V3 actually unus the weights um you must unfuse actually I have to like highly suggest you to unfuse the weights you can only fuse them if you want to do training fter um this will make training like maybe 5% faster it's actually not that much it's like two 2% you actually increase memory usage a lot so just be careful of that as well um oh yes they actually did so this is the sliding window one they actually fixed it um and then they unfixed it um I think they just forgot about it I'll probably like push them again to fix it um and this is the fusing of the weights um so we show that if you actually unfuse the weights so qk and V must be separate you must not combine them um if you combine them you actually have lower accuracy so please do not do that um for tokenization remember this slide which I show you about the um the smiley faces are like the spaces and each one's a different tokenization there are actually many issues um for tokenization um this is a totally different SE separate topic from finding bugs and issues in language models um this is a whole topic of its own because tokenizers are very problematic um and they're very hard to find and fix did I double this slide okay I double that um and also we have new support which we have not announced yet which you can try out so lots of people have asked us for how do we actually fine-tune a language model and Export it to AMA um effectively um does does people do know what's Ama or no or does anyone not know what's okay so AMA is like a interface when you when you find a model you have to run it right you have to run the model somewhere and just makes you run the model much easier um so like youit chbt PT is like the running mechanism AMA is just like chbt but they don't have the model you have to select a model um that's kind ofama um yes how did you manage to like um so I've been working on converting uh creating model files using U um the uh automated pipeline but we've been found many issues trying to automate model file creation is this using unslot no using Axel or something or other ones did you automat modify yourself or well we yeah we because we need we need our own model files right also we do this automatically now so we've un we actually we spent I spent like a few one month on trying to automate the model file creation why we were struggling so hard as a company yes um uh we I have code for that somewhere um okay open SCE oh yeah it's already in the GitHub repost if you go to unso um you go to chat templates we have code for that Lama it is feel very ugly um so these are the chat templates for remember the BOS token someone mentioned you have to add it um yeah add the BOS token this is the oama chat template um which we so AMA has a Umama has a specific requirement is you must have a chat template because if you don't use a correct chat template your model will output incorrect like substandard responses um so this is the chat template for like some of them I had to we had to write chat templates for all of the architectures um and we have an automatic one so these are bakuna and blah blah blah um alpaca skit style um Jemma the JMA style we also have that um we have many many even a llama 3 chat template we have as well um now for the automatic one so what we do is we can actually make an automatic chat template a modifier for you automatically um and this makes your fine tune much more accurately um wait I'll show you the where is the code for that um where is the code okay you can see the code is quite large for the just the chat templates right this is just for tokenization so it's not even the yes this is aache 2.0 right yes it's aache yes it's it's open source yeah um wait where is it okay so we have something called pass combined prompt which does some open squar it didn't actually optimize this it does over squ I should have done o of n but anyways it's O of n squ um checking the prompt um here's the prompt format so we do it looks quite ugly the code for automatic model file creation but we actually made it so you can actually Auto automatically creates a modile from your chat template um you can see it's quite ugly um but it works um and uh yes oh it's even more ugly y it's quite ugly code um but unfortunately the model file is very hard to create automatically and so we have the notebook which allows you to do this um so so this notebook is in here alpaca so this one's for the alpaca data set and so this is our installation llama 3 uh where is it so we we so we'll be using alpaca gbd4 um gbd4 data set so you use alpaca data set and use gbd4 to create the data set um and the trick is though um we also have a CSV file now so you can actually upload a CSV file and use unsoft directly to find you in a language model um and but the problem is a language model must have an instruction and output right only two columns CSV files and Excel files can have many columns colums so what do you do you have to merge The Columns into one um so remember each of those columns in your Excel file convert them into text and for example the Titanic data set you merge them to say they have one siblings and spouses and so on right you merge the row into one row um and that's what you do and with unstop you can do this now it's I still probably need to edit the um the like syntax calling but this merging technique says okay your First Column is called an instruction column and the two double brackets means it's optional um so if if the input column exists then it will say the instruction followed by your input is um and you can like make this very crazy you can do as many columns as you like um I don't know if the syntax is useful but like I will probably be editing this we're going to make a YouTube video about this to talk about this um this is actually very important for fine tuning um we noticed that every single provider requires you to use only one column for instruction and one output column now you can have infinite columns well how many you like but you must Define the chat template um and and we also have a custom customizable chat template um so before when you do F tuning of language models you have to use our Packa prompt um in our other notebooks right below is an instruction that describes a task paired with an input blah blah blah you put your instruction here you put your input here and you put your output here right but notice what is the problem with this is there a problem with this so you must only put one instruction and one output or response right the input is a problem right so how do you solve this you solve this by merging the input into your instruction prompt right so this actually should be removed entirely right and your your input should be something else and what you do is we can actually you now we can do this now right so you must do you must put the input and you must put an output right you can only use two columns now but you can use remember even though you can only use two columns you can use this to convert your data set into two columns um yes do you lose any of the semantic meanings though oh no I don't think so no you don't think so no I don't think so it depends on how you it depends on how you format the data set remember it's a language model so you can do the more you tell the language model what to do the better of course yeah but the problem is to do the model file creation you must do two iterations repetitions of this right you must do instruction response and then you do another one you must okay you must do this for unso I found this to be very very important for the model file creation if you do not do this you have dangling new lines and you actually make your model output terrible um so you must do two repetitions of this okay it's a must must um and if you don't do that we'll error out um and so once you do this we also have examples of for example this is Lama 3's chat template right we again do two iterations you must do two iterations most important um and when you finish training the model um remember you can do runtime run all um you can do inference now right continue the Fibonacci sequence your input is 1 one2 whatever and the next Mar sequence is 13 I think that's correct yes that's correct um so your language model has learned how to do Fibonacci and because it's a chat template you can also do you can shove in multiple messages into the model um so this becomes a chat gbt for you this is the customized chat gbt that you can use um and finally when you want to save the model um you can save it to lower adapters so this only is 100 MB in size so once you f to the model you have 100 m b but some people also want to like merge the model back um and that will take 16 GB um but you must merge this for like Umama support and ggw and stuff like that and what we sh for AMA support is you first have to like you know install oama um you select what you want to save the model to ggw so this is 8bit um we now support multiple quantization methods right you don't have to do eight bit you can do like four bit five bit whatever you like and this will be saved into one go much faster um in fact I think this will save you like 20 minutes of your time um and we save this automatically um okay and this does all the saving blah blah blah saves and we also you see we automatically create an model file automatically using your chat chat template and I can verify this is actually correct because I tried it um and then when you want to serve the model file you can actually print out the model file which we created and this is the model file um whoops I pressed run already um anyways um and finally to serve it you can just do model file to serve it um and you can serve this um and we do have a CSV version so you can actually use the Titanic data set um okay it's loading um so if you want to use a Titanic data set you can upload the Titanic data set right we I upload the tianic CSV you can use the Cs CSV file for this um and again you have the merger columns and so on right this is a more complicated example um in fact I provide this entire example for you for the entire Titanic data set to merge all the columns into one um and it's the same exact output so that's a notebook that we're sharing for we did not release this yet so this is for you guys to like experiment and see if there's any issues um yeah and just tell me um we also have blog posts on our website which you can see um our unsu GitHub repo um and we have stickers available um and they're very very cute for you to take and than yeah and also yeah we have Q&A now yeah yes oh did you meure did you measure the difference makes to write the CSV content in English sentences as opposed to just D the put Jason format the problem is if you put them in Json format you still need to have instruction and output so how would you do that you need to have two columns only for fine tuning can you do like in your template here you have instruction and then you add all yeah you add all the other colums onto it can you just instructions yes you could you could do the Json file yes you can um but we just show you that you can do multiple columns now so like if you have like 10 columns you can now make the 10 columns into one um by merging them together does that kind there's a big difference in representing that merge columns as an English sentence for like a dictionary oh no you can't use you mean like you shove the actual dictionary for fine tuning I you could do that I don't I think you should do English language because a language model predicts the next word Jason is probably less useful always convert it into English I have the same intuition I was wondering if you measured research paper yes it should be another research paper yeah any other questions so there's a lot of upvoted questions from me on the chat sorry it's oh I was wondering if you could take a look at them I oh yeah I oh yeah I didn't actually check the slider questions whoopsies um it actually didn't load so oh there's lots of questions okay I will okay oh okay oh okay oh okay I need to I need to um answer each of them afterwards I think I'm already out of time though so yes thanks a lot [Music]

--------------------------------

Generated for FREE with Youtube to Text: https://www.youtubetotext.org

Support our services by donating: https://link.youtubetotext.org/make-a-donation

Thanks for using our services!

